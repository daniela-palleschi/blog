[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is run by Daniela Palleschi, a researcher and lecturer at the Institute of German Language and Linguistics (Institut für deutsche Sprache und Linguistik) at the Humboldt-Universität zu Berlin. Her current position includes teaching statistics and quantitative methods, as well as providing consultations on these topics to students.\nBlog posts are typically written with students in mind, with the aim of addressing common problems that they seem to have."
  },
  {
    "objectID": "future-posts/my-post/index.html",
    "href": "future-posts/my-post/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "future-posts/repro/index.html",
    "href": "future-posts/repro/index.html",
    "title": "Reproducibility 101",
    "section": "",
    "text": "Bryan, Jenny, and Jim Hester. What They Forgot to Teach You About R. (2019). url: https://rstats.wtf/\nBryan, Jenny (). Happy Git and GitHub for the useRhttps://happygitwithr.com/\nBlogpost An Introduction to Docker for R Users by Colin Fay https://colinfay.me/docker-r-reproducibility/\nReproducible Work in R: Why and How to use Docker in your daily R workflow (2019) by Rahul S. https://towardsdatascience.com/reproducible-work-in-r-e7d160d5d198\n\n\n\n\n[here]\n[renv]\n[targets]\n[docker]"
  },
  {
    "objectID": "future-posts/repro/index.html#reading",
    "href": "future-posts/repro/index.html#reading",
    "title": "Reproducibility 101",
    "section": "",
    "text": "Bryan, Jenny, and Jim Hester. What They Forgot to Teach You About R. (2019). url: https://rstats.wtf/\nBryan, Jenny (). Happy Git and GitHub for the useRhttps://happygitwithr.com/\nBlogpost An Introduction to Docker for R Users by Colin Fay https://colinfay.me/docker-r-reproducibility/\nReproducible Work in R: Why and How to use Docker in your daily R workflow (2019) by Rahul S. https://towardsdatascience.com/reproducible-work-in-r-e7d160d5d198"
  },
  {
    "objectID": "future-posts/repro/index.html#packages",
    "href": "future-posts/repro/index.html#packages",
    "title": "Reproducibility 101",
    "section": "",
    "text": "[here]\n[renv]\n[targets]\n[docker]"
  },
  {
    "objectID": "future-posts/ggeffects/index.html",
    "href": "future-posts/ggeffects/index.html",
    "title": "ggeffects Package",
    "section": "",
    "text": "When reporting our models, it can be helpful for our audience (and ourselves) to produce model predicted values per condition/values of a continuous predictor. Using the equation of a line, we can manually generate such predictions as long as we know:\nThere are also useful packages that automate this process for us, which can be especially useful with more complex models such as mixed effects models.\nWe’ll see here how to manually compute the fitted values per condition, and how these compare to functions from the ggeffects package."
  },
  {
    "objectID": "future-posts/ggeffects/index.html#fit-model",
    "href": "future-posts/ggeffects/index.html#fit-model",
    "title": "ggeffects Package",
    "section": "Fit model",
    "text": "Fit model\n\nglmer_dative &lt;- \n  glmer(\n  realization_of_recipient ~ semantic_class + length_of_recipient +\n    (1 | verb),\n  # control = glmerControl(optimizer = \"optimx\", optCtrl = list(method = \"nlminb\")),\n  data = df_dative,\n  family = \"binomial\"\n)\n\n\nsummary(glmer_dative)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: realization_of_recipient ~ semantic_class + length_of_recipient +  \n    (1 | verb)\n   Data: df_dative\n\n     AIC      BIC   logLik deviance df.resid \n  1710.4   1750.7   -848.2   1696.4     2353 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-33.155  -0.443  -0.272  -0.042   5.764 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n verb   (Intercept) 1.715    1.31    \nNumber of obs: 2360, groups:  verb, 38\n\nFixed effects:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -3.05370    0.35904  -8.505   &lt;2e-16 ***\nsemantic_classc      0.07234    0.31981   0.226   0.8210    \nsemantic_classf      0.53767    0.66759   0.805   0.4206    \nsemantic_classp     -3.68208    1.43821  -2.560   0.0105 *  \nsemantic_classt      0.97386    0.16237   5.998    2e-09 ***\nlength_of_recipient  0.98917    0.08216  12.039   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) smntc_clssc smntc_clssf smntc_clssp smntc_clsst\nsmntc_clssc -0.307                                                \nsmntc_clssf -0.280  0.098                                         \nsmntc_clssp -0.185  0.055       0.050                             \nsmntc_clsst -0.253  0.261       0.054       0.070                 \nlngth_f_rcp -0.391  0.077       0.021       0.011       0.085"
  },
  {
    "objectID": "future-posts/ggeffects/index.html#calculate-predictions",
    "href": "future-posts/ggeffects/index.html#calculate-predictions",
    "title": "ggeffects Package",
    "section": "Calculate predictions",
    "text": "Calculate predictions\nFirst, remind ourselves of our contrasts\n\ncontrasts(df_dative$semantic_class)\n\n  c f p t\na 0 0 0 0\nc 1 0 0 0\nf 0 1 0 0\np 0 0 1 0\nt 0 0 0 1\n\n\nGrab our estimates.\n\ntidy(glmer_dative)\n\n# A tibble: 7 × 7\n  effect   group term                estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;  (Intercept)          -3.05      0.359     -8.51   1.81e-17\n2 fixed    &lt;NA&gt;  semantic_classc       0.0723    0.320      0.226  8.21e- 1\n3 fixed    &lt;NA&gt;  semantic_classf       0.538     0.668      0.805  4.21e- 1\n4 fixed    &lt;NA&gt;  semantic_classp      -3.68      1.44      -2.56   1.05e- 2\n5 fixed    &lt;NA&gt;  semantic_classt       0.974     0.162      6.00   2.00e- 9\n6 fixed    &lt;NA&gt;  length_of_recipient   0.989     0.0822    12.0    2.22e-33\n7 ran_pars verb  sd__(Intercept)       1.31     NA         NA     NA       \n\n\nSave them as objects to make things a little more transparent.\n\nintercept &lt;-\n  tidy(glmer_dative) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nsem_class_a_c &lt;-\n  tidy(glmer_dative) |&gt; \n  filter(term == \"semantic_classc\") |&gt; \n  pull(estimate)\n\nsem_class_a_f &lt;-\n  tidy(glmer_dative) |&gt; \n  filter(term == \"semantic_classf\") |&gt; \n  pull(estimate)\n\nsem_class_a_p &lt;-\n  tidy(glmer_dative) |&gt; \n  filter(term == \"semantic_classp\") |&gt; \n  pull(estimate)\n\nsem_class_a_t &lt;-\n  tidy(glmer_dative) |&gt; \n  filter(term == \"semantic_classt\") |&gt; \n  pull(estimate)\n\nlength_recip &lt;- \n  tidy(glmer_dative) |&gt; \n  filter(term == \"length_of_recipient\") |&gt; \n  pull(estimate)\n\nNow we can manually calculate:\n\na &lt;- plogis(intercept + sem_class_a_c*0 + length_recip*1)\n\n\nc &lt;- plogis(intercept + sem_class_a_c*1 + length_recip*1)\n\n\nf &lt;- plogis(intercept + sem_class_a_f*1 + length_recip*1)\n\n\np &lt;- plogis(intercept + sem_class_a_p*1 + length_recip*1)\n\n\nt &lt;- plogis(intercept + sem_class_a_t*1 + length_recip*1)\n\n\na\n\n[1] 0.1125924\n\nc\n\n[1] 0.1200255\n\nf\n\n[1] 0.1784539\n\np\n\n[1] 0.003183409\n\nt\n\n[1] 0.2514916\n\n\n\nggeffects::ggpredict(glmer_dative, terms = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.11 | 0.06, 0.19\nc              |      0.12 | 0.06, 0.22\nf              |      0.18 | 0.06, 0.43\np              |      0.00 | 0.00, 0.05\nt              |      0.25 | 0.16, 0.38\n\nAdjusted for:\n* length_of_recipient = 1.00\n*                verb = 0 (population-level)\n\n\nThis matches our manual calculations.\n\nggeffects::ggeffect(glmer_dative, terms = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.16 | 0.09, 0.26\nc              |      0.17 | 0.09, 0.30\nf              |      0.24 | 0.08, 0.53\np              |      0.00 | 0.00, 0.07\nt              |      0.33 | 0.20, 0.49\n\n\nThis doesn’t…why is that?\n\nggeffects::ggemmeans(glmer_dative, terms = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.11 | 0.06, 0.20\nc              |      0.12 | 0.06, 0.22\nf              |      0.18 | 0.06, 0.44\np              |      0.00 | 0.00, 0.05\nt              |      0.25 | 0.15, 0.39\n\nAdjusted for:\n* length_of_recipient = 1.00\n\n\nThis also matches our predictions."
  },
  {
    "objectID": "future-posts/ggeffects/index.html#centre-predictors",
    "href": "future-posts/ggeffects/index.html#centre-predictors",
    "title": "ggeffects Package",
    "section": "Centre predictors",
    "text": "Centre predictors\n\ncontrasts(df_dative$semantic_class)\n\n  c f p t\na 0 0 0 0\nc 1 0 0 0\nf 0 1 0 0\np 0 0 1 0\nt 0 0 0 1\n\ncontrasts(df_dative$semantic_class) &lt;- contr.sum(5)/2\ncontrasts(df_dative$semantic_class)\n\n  [,1] [,2] [,3] [,4]\na  0.5  0.0  0.0  0.0\nc  0.0  0.5  0.0  0.0\nf  0.0  0.0  0.5  0.0\np  0.0  0.0  0.0  0.5\nt -0.5 -0.5 -0.5 -0.5\n\n\n\nsummary(df_dative$length_of_recipient)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.386   1.000  15.000 \n\nhist(df_dative$length_of_recipient)\n\n\n\n\n\n\n\n\n\ndf_dative &lt;-\n  df_dative |&gt; \n  mutate(length_of_recipient_z = scale(df_dative$length_of_recipient))\n\n\ndf_dative |&gt; \n  select(length_of_recipient, length_of_recipient_z) |&gt; \n  head()\n\n    length_of_recipient length_of_recipient_z\n903                   1            -0.3283482\n904                   2             0.5213247\n905                   1            -0.3283482\n906                   2             0.5213247\n907                   2             0.5213247\n908                   1            -0.3283482"
  },
  {
    "objectID": "future-posts/ggeffects/index.html#fit-model-1",
    "href": "future-posts/ggeffects/index.html#fit-model-1",
    "title": "ggeffects Package",
    "section": "Fit model",
    "text": "Fit model\n\nglmer_dative_c &lt;- \n  glmer(\n  realization_of_recipient ~ semantic_class + length_of_recipient +\n    (1 | verb),\n  # control = glmerControl(optimizer = \"optimx\", optCtrl = list(method = \"nlminb\")),\n  data = df_dative,\n  family = \"binomial\"\n)\n\n\nsummary(glmer_dative_c)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: realization_of_recipient ~ semantic_class + length_of_recipient +  \n    (1 | verb)\n   Data: df_dative\n\n     AIC      BIC   logLik deviance df.resid \n  1710.4   1750.7   -848.2   1696.4     2353 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-33.155  -0.443  -0.272  -0.042   5.764 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n verb   (Intercept) 1.715    1.31    \nNumber of obs: 2360, groups:  verb, 38\n\nFixed effects:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -3.47333    0.39977  -8.688  &lt; 2e-16 ***\nsemantic_class1      0.83925    0.68060   1.233  0.21754    \nsemantic_class2      0.98392    0.78282   1.257  0.20880    \nsemantic_class3      1.91463    1.18820   1.611  0.10710    \nsemantic_class4     -6.52475    2.29360  -2.845  0.00444 ** \nlength_of_recipient  0.98917    0.08216  12.039  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) smnt_1 smnt_2 smnt_3 smnt_4\nsmntc_clss1 -0.539                            \nsmntc_clss2 -0.486  0.626                     \nsmntc_clss3 -0.152  0.057  0.000              \nsmntc_clss4  0.570 -0.805 -0.715 -0.543       \nlngth_f_rcp -0.317 -0.040  0.028  0.001  0.002\n\n\n\ntidy(glmer_dative_c)\n\n# A tibble: 7 × 7\n  effect   group term                estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;  (Intercept)           -3.47     0.400      -8.69  3.68e-18\n2 fixed    &lt;NA&gt;  semantic_class1        0.839    0.681       1.23  2.18e- 1\n3 fixed    &lt;NA&gt;  semantic_class2        0.984    0.783       1.26  2.09e- 1\n4 fixed    &lt;NA&gt;  semantic_class3        1.91     1.19        1.61  1.07e- 1\n5 fixed    &lt;NA&gt;  semantic_class4       -6.52     2.29       -2.84  4.44e- 3\n6 fixed    &lt;NA&gt;  length_of_recipient    0.989    0.0822     12.0   2.21e-33\n7 ran_pars verb  sd__(Intercept)        1.31    NA          NA    NA       \n\n\nOur estimates have changed.\nLet’s first take a look at the predictions computed by the ggeffects package.\n\nggpredict(glmer_dative_c, term = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.11 | 0.06, 0.19\nc              |      0.12 | 0.06, 0.22\nf              |      0.18 | 0.06, 0.43\np              |      0.00 | 0.00, 0.05\nt              |      0.25 | 0.16, 0.38\n\nAdjusted for:\n* length_of_recipient = 1.00\n*                verb = 0 (population-level)\n\n\nThese are the same as before.\n\nggeffect(glmer_dative_c, term = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.16 | 0.09, 0.26\nc              |      0.17 | 0.09, 0.30\nf              |      0.24 | 0.08, 0.53\np              |      0.00 | 0.00, 0.07\nt              |      0.33 | 0.20, 0.49\n\n\nThese are also the same as before.\n\nggemmeans(glmer_dative_c, term = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.11 | 0.06, 0.20\nc              |      0.12 | 0.06, 0.22\nf              |      0.18 | 0.06, 0.44\np              |      0.00 | 0.00, 0.05\nt              |      0.25 | 0.15, 0.39\n\nAdjusted for:\n* length_of_recipient = 1.00\n\n\nThese are again the same as before, and match those computed by ggpredict().\nSo what’s going on here? To understand it better, let’s first repeat the manual calculations using the model estimates."
  },
  {
    "objectID": "future-posts/ggeffects/index.html#manual-predictions",
    "href": "future-posts/ggeffects/index.html#manual-predictions",
    "title": "ggeffects Package",
    "section": "Manual predictions",
    "text": "Manual predictions\nFirst, remind ourselves of our contrasts\n\ncontrasts(df_dative$semantic_class)\n\n  [,1] [,2] [,3] [,4]\na  0.5  0.0  0.0  0.0\nc  0.0  0.5  0.0  0.0\nf  0.0  0.0  0.5  0.0\np  0.0  0.0  0.0  0.5\nt -0.5 -0.5 -0.5 -0.5\n\n\nGrab our estimates.\n\ntidy(glmer_dative_c)\n\n# A tibble: 7 × 7\n  effect   group term                estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;  (Intercept)           -3.47     0.400      -8.69  3.68e-18\n2 fixed    &lt;NA&gt;  semantic_class1        0.839    0.681       1.23  2.18e- 1\n3 fixed    &lt;NA&gt;  semantic_class2        0.984    0.783       1.26  2.09e- 1\n4 fixed    &lt;NA&gt;  semantic_class3        1.91     1.19        1.61  1.07e- 1\n5 fixed    &lt;NA&gt;  semantic_class4       -6.52     2.29       -2.84  4.44e- 3\n6 fixed    &lt;NA&gt;  length_of_recipient    0.989    0.0822     12.0   2.21e-33\n7 ran_pars verb  sd__(Intercept)        1.31    NA          NA    NA       \n\n\nSave them as objects to make things a little more transparent.\n\nintercept &lt;-\n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nsem_class_t_a &lt;-\n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"semantic_class1\") |&gt; \n  pull(estimate)\n\nsem_class_t_c &lt;-\n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"semantic_class2\") |&gt; \n  pull(estimate)\n\nsem_class_t_f &lt;-\n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"semantic_class3\") |&gt; \n  pull(estimate)\n\nsem_class_t_p &lt;-\n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"semantic_class4\") |&gt; \n  pull(estimate)\n\nlength_recip &lt;- \n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"length_of_recipient\") |&gt; \n  pull(estimate)\n\nNow we can manually calculate:\n\n# t &lt;- \n  plogis(intercept + sem_class_t_a*(-0.5))\n\n[1] 0.01997777\n\nplogis(intercept + sem_class_t_p*(-0.5))\n\n[1] 0.4474558\n\nplogis(intercept + sem_class_t_f*(-0.5))\n\n[1] 0.01176666\n\nplogis(intercept + sem_class_t_c*(-0.5))\n\n[1] 0.01860966\n\n\n\nsummary(df_dative$semantic_class)\n\n  a   c   f   p   t \n907 371  47 183 852 \n\n\n\na &lt;- plogis(intercept + sem_class_t_a*(+0.5) + length_recip*0)\n\n\nf &lt;- plogis(intercept + sem_class_t_f*(+0.5) + length_recip*0)\n\n\np &lt;- plogis(intercept + sem_class_t_p*(+0.5) + length_recip*0)\n\n\nc &lt;- plogis(intercept + sem_class_t_c*(+0.5) + length_recip*0)\n\n\na\n\n[1] 0.04505766\n\nc\n\n[1] 0.04827448\n\nf\n\n[1] 0.07474313\n\np\n\n[1] 0.001186326\n\nt\n\n[1] 0.2514916"
  },
  {
    "objectID": "future-posts/ggeffects/index.html#difference-between-ggeffect-ggpredict-ggemmeans",
    "href": "future-posts/ggeffects/index.html#difference-between-ggeffect-ggpredict-ggemmeans",
    "title": "ggeffects Package",
    "section": "Difference between ggeffect(), ggpredict(), ggemmeans()",
    "text": "Difference between ggeffect(), ggpredict(), ggemmeans()"
  },
  {
    "objectID": "posts/dotplot/index.html",
    "href": "posts/dotplot/index.html",
    "title": "Visualising random effects",
    "section": "",
    "text": "This blog post goes over how to produce visualisations of by-grouping factor (e.g., by-participant) varying intercepts and slopes."
  },
  {
    "objectID": "posts/dotplot/index.html#packages",
    "href": "posts/dotplot/index.html#packages",
    "title": "Visualising random effects",
    "section": "Packages",
    "text": "Packages\nRequired packages for reproducible example:\n\n## install 'pacman' if you don't have it\n# install.packages(\"pacman\")\n\n# p_load(): load or install all listed CRAN packages\npacman::p_load(\n  languageR, # for the example data\n  dplyr, # summarising our data\n  janitor, # for summarising our data\n  lme4, # fitting our model\n  lattice # for the caterpillar plots\n)"
  },
  {
    "objectID": "posts/dotplot/index.html#load-data",
    "href": "posts/dotplot/index.html#load-data",
    "title": "Visualising random effects",
    "section": "Load data",
    "text": "Load data\nWe’ll use a dataset from the languageR dataset (Baayen & Shafaei-Bajestan, 2019).\n\ndf_lexdec &lt;- languageR::lexdec\n\n\nInspect dataset\nCheck out the variable names:\n\nnames(df_lexdec)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"      \"FamilySize\"     \"SynsetCount\"   \n[13] \"Length\"         \"Class\"          \"FreqSingular\"   \"FreqPlural\"    \n[17] \"DerivEntropy\"   \"Complex\"        \"rInfl\"          \"meanRT\"        \n[21] \"SubjFreq\"       \"meanSize\"       \"meanWeight\"     \"BNCw\"          \n[25] \"BNCc\"           \"BNCd\"           \"BNCcRatio\"      \"BNCdRatio\"     \n\n\nLet’s look at the first few rows of some selected columns:\n\nhead(df_lexdec[c(1:5,9)])\n\n  Subject       RT Trial Sex NativeLanguage       Word\n1      A1 6.340359    23   F        English        owl\n2      A1 6.308098    27   F        English       mole\n3      A1 6.349139    29   F        English     cherry\n4      A1 6.186209    30   F        English       pear\n5      A1 6.025866    32   F        English        dog\n6      A1 6.180017    33   F        English blackberry\n\n\nHow many participants?\n\nlength(unique(df_lexdec$Subject))\n\n[1] 21\n\n\nThere are 21 participants in this dataset. How many unique words were there?\n\nlength(unique(df_lexdec$Word))\n\n[1] 79\n\n\nThere are 79 words.\nHow many observations (rows) per participant?\n\ndf_lexdec |&gt; \n  dplyr::count(Subject) |&gt; \n  count(n, name = \"Subject\")\n\n   n Subject\n1 79      21\n\n\nWe see that all 21 participants had 79 observations, i.e., this was a repeated measures design. This is nice and clean, as 79 x 21 equals the number of observations in the dataset (1659):\n\n79*21 == nrow(df_lexdec)\n\n[1] TRUE\n\n\nThis means there are no missing values. Wouldn’t have been a problem for our model, but is good to know (and always good to double check)."
  },
  {
    "objectID": "posts/dotplot/index.html#fit-mixed-effects-model",
    "href": "posts/dotplot/index.html#fit-mixed-effects-model",
    "title": "Visualising random effects",
    "section": "Fit mixed effects model",
    "text": "Fit mixed effects model\nWe’ll ignore a lot of steps for the sake of simplicity here (e.g., transforming variables). I will just set the two-level factor NativeLanguageOther to sum contrast coding.\n\nlevels(df_lexdec$NativeLanguage)\n\n[1] \"English\" \"Other\"  \n\n\nI see that the level English comes first. I want to set this to -0.5, and Other to +0.5. I do that like so:\n\ncontrasts(df_lexdec$NativeLanguage) &lt;- c(-0.5, +0.5)\n\nAnd I check the contrasts:\n\ncontrasts(df_lexdec$NativeLanguage)\n\n        [,1]\nEnglish -0.5\nOther    0.5\n\n\nLet’s fit a linear mixed model with log reaction times (RT) as predicted by word frequency (Frequency), with by-participant (Subject) and by-word (Word) varying intercepts and slopes.\n\nmod_lexdec &lt;-\n  lmer(RT ~ Frequency + NativeLanguage +\n         (1 + Frequency | Subject) +\n         (1 + NativeLanguage | Word),\n       data = df_lexdec,\n       control = lmerControl(optimizer = \"bobyqa\")\n       )\n\nFirst: note we have a single varying slope for participant and word each. The effect of Frequency is allowed to vary between participants because each participant contributed data points for words of different frequencies (presumably). The effect of NativeLanguage is allowed to vary between words because we have data points from different NativeLanguage levels (English, Other) for each word (presumably). Importantly, NativeLanguage does not vary within a single subject, nor does word frequency vary within a single word.\n\nInspect model\n\nsummary(mod_lexdec)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Frequency + NativeLanguage + (1 + Frequency | Subject) +  \n    (1 + NativeLanguage | Word)\n   Data: df_lexdec\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: -964.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.3977 -0.6132 -0.1193  0.4740  6.2917 \n\nRandom effects:\n Groups   Name            Variance  Std.Dev. Corr \n Word     (Intercept)     0.0032648 0.05714       \n          NativeLanguage1 0.0014730 0.03838  0.87 \n Subject  (Intercept)     0.0462392 0.21503       \n          Frequency       0.0003824 0.01956  -0.88\n Residual                 0.0288063 0.16972       \nNumber of obs: 1659, groups:  Word, 79; Subject, 21\n\nFixed effects:\n                 Estimate Std. Error t value\n(Intercept)      6.580171   0.054611 120.492\nFrequency       -0.039879   0.007122  -5.599\nNativeLanguage1  0.078591   0.054036   1.454\n\nCorrelation of Fixed Effects:\n            (Intr) Frqncy\nFrequency   -0.842       \nNativeLngg1  0.015  0.103\n\n\nIn terms of fixed effects, we see a negative slope for Frequency, indicating shorter log reaction times for more frequent words. We see a positive slope for NativeLanguageOther, indicating longer log reaction times for participants whose first language was not English. We will not consider the size of these effects nor whether they’re “significant”. We’re really interested in the random effects here."
  },
  {
    "objectID": "posts/dotplot/index.html#visualising-random-effects-deviances-from-the-population-level-estimates",
    "href": "posts/dotplot/index.html#visualising-random-effects-deviances-from-the-population-level-estimates",
    "title": "Visualising random effects",
    "section": "Visualising random effects: deviances from the population-level estimates",
    "text": "Visualising random effects: deviances from the population-level estimates\nFinally, the point of this blog post: visualising the random effects.\nPrint just the by-subject random effects deviances:\n\ndotplot(ranef(mod_lexdec))$Subject\n\n\n\n\n\n\n\nFigure 1: By-subject random effects deviances\n\n\n\n\n\nPrint just the by-word random effects:\n\ndotplot(ranef(mod_lexdec))$Word\n\n\n\n\n\n\n\nFigure 2: By-word random effects deviances\n\n\n\n\n\nIn these plots, 0 on the x-axis corresponds to the population-level values. So, the (Intercept), 0 corresponds to the intercept value in the model coefficients (6.5801714):\n\nfixef(mod_lexdec)\n\n    (Intercept)       Frequency NativeLanguage1 \n     6.58017141     -0.03987868      0.07859106 \n\n\nFor most cases, we can just stop here. These plots give us everything we need.\n\n\n\n\n\n\nDeviances from population-level\n\n\n\nThese plots show the deviance from the population-level estimates (where 0 means a mean equal to the population-level intercept, and a value of 0.15 mean a mean RT 0.15 log milliseconds longer than the population-level intercept). That’s why the x-axis is centred around 0.0: this is the population-level value for the relevant plot (either the intercept or native language).\nThe by-participant intercept values are roughly the mean values per participant (or, more precisely, the fitted estimates, which will typically vary a little bit from the exact observed by-participant means due to shrinkage). The slope values would be the effect of the relevant predictor per participant. To get the actual fitted values, we just add the"
  },
  {
    "objectID": "posts/dotplot/index.html#caterpillar-plot-with-ggplot2",
    "href": "posts/dotplot/index.html#caterpillar-plot-with-ggplot2",
    "title": "Visualising random effects",
    "section": "Caterpillar plot with ggplot2",
    "text": "Caterpillar plot with ggplot2\nWe could also produce these plots by hand to (potentially) facilitate the interpretation of the random effects. We won’t really get any new information here, but we can visualise the same information differently. These steps require more experience with the infrastructure of an lmer() model, as well as dplyr, tidyr, and ggplot2 syntax. You can ignore the coding part and just look at the produced plots to see if these aid in the interpretation of the random effects.\nWe’ll use the broom.mixed and ggplot2 packages.\n\npacman::p_load(\n  ggplot2,\n  broom.mixed\n)\n\n\nfig_res_dev &lt;-\n  # produce tidy table of random effects\n  broom.mixed::tidy(mod_lexdec, effects = \"ran_vals\", conf.int = TRUE) |&gt; \n  # by-Subject only\n  filter(group == \"Subject\") |&gt; \n  # begin plotting\n  ggplot() +\n  aes(x = estimate, y = reorder(level, estimate))  +\n  geom_errorbar(\n    aes(xmin = conf.low,\n        xmax = conf.high)\n  ) +\n  geom_point(colour = \"blue\") +\n  # add vertical line for population-level intercept\n  geom_vline(xintercept = 0, colour = \"red\", linetype = \"dashed\", alpha = .5) +\n  # add informative labels\n  labs(title = \"By-participant intercept deviance (log RTs)\",\n       y = \"Participant ID\",\n       x = \"Deviance (log ms)\") +\n  facet_grid(~term) +\n  theme_bw()\n\n\nfig_res_dev\n\n\n\n\n\n\n\nFigure 3: Caterpillar plot of by-participant varying intercepts and slopes (deviances) produced with broom.mixed::tidy() and ggplot2\n\n\n\n\n\n\nVisualising fitted values\nWe can also shift the x-axis by the population-level estimates so that we have the fitted random effects values rather than the deviances from the population-level estimates. We do this by adding a line of code that (see code annotation 1. below) and moving the red dotted line to cross at the population-level intercept value (see code annodtation 2. below). We’ll do this by directly accessing the Intercept value from our model using:\n\nfixef(mod_lexdec)[1]\n\n(Intercept) \n   6.580171 \n\n\n\nfig_res_b0 &lt;-\n  # produce tidy table of random effects\n  broom.mixed::tidy(mod_lexdec, effects = \"ran_vals\", conf.int = TRUE) |&gt; \n1  mutate(across(c(estimate,conf.low,conf.high),~.+fixef(mod_lexdec)[1])) |&gt;\n  # by-Subject varying intercepts only\n  filter(group == \"Subject\",\n2         term == \"(Intercept)\") |&gt;\n  # begin plotting\n  ggplot() +\n  aes(x = estimate, y = reorder(level, estimate)) +\n  geom_errorbar(\n    aes(xmin = conf.low,\n        xmax = conf.high)\n  ) +\n  geom_point(colour = \"blue\") +\n  # add vertical line for population-level intercept\n3  geom_vline(xintercept = fixef(mod_lexdec)[1],\n             colour = \"red\", linetype = \"dashed\", alpha = .5) +\n  # add informative labels\n  labs(title = \"By-participant varying intercepts\",\n       y = \"Participant ID\",\n       x = \"Intercept (log ms)\") +\n  # beautify\n  theme_bw()\n\n\n1\n\nAdd the intercept value to all the by-participant intercept deviances\n\n2\n\nLook only at intercepts\n\n3\n\nAdd the red dotted line to the population-level intercept value\n\n\n\n\nWe do this separately for the varying intercept and for the varying slope(s) because the population-levels we add differ. Now we want to adjust each by-participant varying slope (Frequency) by replacing fixef(mod_lexdec)[1] with fixef(mod_lexdec)[2].\n\nfixef(mod_lexdec)[2]\n\n  Frequency \n-0.03987868 \n\n\n\nfig_res_b1 &lt;-\n  # produce tidy table of random effects\n  broom.mixed::tidy(mod_lexdec, effects = \"ran_vals\", conf.int = TRUE) |&gt; \n  mutate(across(c(estimate,conf.low,conf.high),~.+fixef(mod_lexdec)[2])) |&gt; # (1) \n  # by-Subject varying intercepts only\n  filter(group == \"Subject\",\n         term == \"Frequency\") |&gt; # (2) \n  # begin plotting\n  ggplot() +\n  aes(x = estimate, y = reorder(level, estimate)) +\n  geom_errorbar(\n    aes(xmin = conf.low,\n        xmax = conf.high)\n  ) +\n  geom_point(colour = \"blue\") +\n  # add vertical line for population-level intercept\n  geom_vline(xintercept = fixef(mod_lexdec)[2], # (3) \n             colour = \"red\", linetype = \"dashed\", alpha = .5) +\n  # add informative labels\n  labs(title = \"By-participant varying word-frequency slopes\",\n       y = \"Participant ID\",\n       x = \"Frequency (log ms)\") +\n  # beautify\n  theme_bw()\n\n\nAdd the Frequency estimate to all the by-participant slope deviances\nLook only at frequency only\nAdd the red dotted line to the population-level frequency value\n\nNow we can print them side-by-side\n\n\nCode\nlibrary(patchwork)\n(fig_res_b0 + fig_res_b1) /\n  fig_res_dev +\n  plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\n\nFigure 4: Comparison of manual visualisations of by-participant random effects coefficients (A and B) and random effects deviances (C)"
  },
  {
    "objectID": "posts/dotplot/index.html#interpretation",
    "href": "posts/dotplot/index.html#interpretation",
    "title": "Visualising random effects",
    "section": "Interpretation",
    "text": "Interpretation\nNotice that the two figures for the intercepts are pretty identical, save the linear transformation of the x-axis values. The varying slopes look a little different: this is due to the ordering of the participants, which in Figure 4 B is ordered by the by-participant frequency slope value (e.g., participant Z has the smallest slope value, not to be confused with the magnitude/size of the effect as it’s actually the largest by-participant slope), whereas the right plot in Figure 4 C is ordered by the varying intercept value presented in the left plot in Figure 4 C. This is helpful because it also helps us visualise the correlation between by-participant varying intercepts and slopes, which is -0.877:\n\nVarCorr(mod_lexdec)\n\n Groups   Name            Std.Dev. Corr  \n Word     (Intercept)     0.057138       \n          NativeLanguage1 0.038380 0.874 \n Subject  (Intercept)     0.215033       \n          Frequency       0.019556 -0.877\n Residual                 0.169724       \n\n\nThis negative correlation tells us that, in general, participants who had overall slower reaction times tended to have overall more smaller slope values. Again, be careful interpreting this is a smaller effect! Smaller negative numbers are actually larger negative effects).\nLooking back at Figure 4 B, the confidence intervals are also much wider than in the frequency plot in Figure 4 C. This is simply because of the x-axis range: in the latter, the x-axis range is identical to that of the varying intercepts, which has a broader range. That is to say, there was more within-participant variance in overall reaction times than there was in the overall effect of frequency. Figure 4 B simply has the range of values relevant for the slope values.\nFigure 4 B could be further adapted to mimic the frequency slope figure in Figure 4 C with some extra code (i.e., setting the x-axis range, re-ordering participants by corresponding intercept values). It’s a matter of what exactly you want to get from these visualisations that will decide what you prefer.\n\nfig_res_b1_corr &lt;-\n  # produce tidy table of random effects\n  broom.mixed::tidy(mod_lexdec, effects = \"ran_vals\", conf.int = TRUE) |&gt; \n  mutate(across(c(estimate,conf.low,conf.high),~.+fixef(mod_lexdec)[2])) |&gt; \n  # by-Subject varying intercepts only\n  filter(group == \"Subject\") |&gt; \n  # rename (Intercept)\n  mutate(term = ifelse(term == \"(Intercept)\", \"Intercept\", term)) |&gt;\n  # pivot to create single column for intercept/slope estimates and conf. int's \n  tidyr::pivot_wider( \n    id_cols = c(\"group\", \"level\"), \n    values_from = c(estimate, conf.low, conf.high), \n    names_from = term \n  ) |&gt; \n  # remove unneeded variables\n  select(-conf.low_Intercept, -conf.high_Intercept) |&gt; \n  # rename variables\n  rename(Intercept = estimate_Intercept,\n         estimate = estimate_Frequency,\n         conf.low = conf.low_Frequency,\n         conf.high = conf.high_Frequency) |&gt; \n  # begin plotting\n  ggplot() +\n  # reorder participants (level) by intercept \n  aes(x = estimate, y = reorder(level, Intercept)) + \n  geom_errorbar(\n    aes(xmin = conf.low,\n        xmax = conf.high)\n  ) +\n  geom_point(colour = \"blue\") +\n  # add vertical line for population-level intercept\n  geom_vline(xintercept = fixef(mod_lexdec)[2], # 3. \n             colour = \"red\", linetype = \"dashed\", alpha = .5) +\n  # add informative labels\n  labs(title = \"By-participant varying word-frequency slopes\",\n       y = \"Participant ID\",\n       x = \"Frequency (log ms)\") +\n  # add x-axis ticks with an identical range to the intercept plot \n  scale_x_continuous(limits = c(-.5148797, 0.4351213)) + \n  # beautify\n  theme_bw()\n\n\n\nCode\nlibrary(patchwork)\n  (fig_res_b0 + fig_res_b1_corr) /\n    fig_res_dev +\n  plot_annotation(tag_levels = \"A\")\n\n\n\n\n\n\n\n\nFigure 5: Comparison of manual visualisations of by-participant random effects coefficients ordered by varying intercept estimates (A and B) and random effects deviances (C)\n\n\n\n\n\nIn Figure 5 we see that the varying slope values are ordered by by-participant intercept value (maintaining the correlation visulation), and that the x-axis ranges are comparable. The only difference is htat in Figure 5 B, the population-level slope value (-0.0398787) is centred, whereas in the frequency facet in Figure 5 C, 0 is not quite centred. Let’s compare this one last time to the deviance dotplot we get with lattice::dotplot(ranef()) (Figure 6).\n\nlattice::dotplot(ranef(mod_lexdec))$Subject\n\n\n\n\n\n\n\nFigure 6: lattice::dotplot(ranef(mod_lexdec))$Subject"
  },
  {
    "objectID": "posts/papapja7/index.html",
    "href": "posts/papapja7/index.html",
    "title": "APA 7 in papaja",
    "section": "",
    "text": "This is a short blogpost documenting the steps I took to get APA 7 running in papaja for Rmarkdown. I followed more detailed instructions in the blogpost How to set up {papaja} to work with the APA 7 format, and also took a look at an open issue on the papaja GitHub repo on this topic."
  },
  {
    "objectID": "posts/papapja7/index.html#setting-up-apa-7-in-papaja",
    "href": "posts/papapja7/index.html#setting-up-apa-7-in-papaja",
    "title": "APA 7 in papaja",
    "section": "Setting up APA 7 in papaja",
    "text": "Setting up APA 7 in papaja\npapaja is an R package for preparing APA-styled articles in Rmarkdown. It’s current version (0.1.2) supports APA 6 guidelines, although the newest guidelines are APA 7. Basically, we need to get papaja to use APA 7.\nI achieved this by following these steps:\n\nInstall developer version of papaja:\n\n\nremotes::install_github(\"crsh/papaja@devel\")\n\n\nAdd to the YAML:\n\n\ncsl               : \"`r system.file('rmd', 'apa7.csl', package = 'papaja')`\"\ndocumentclass     : \"apa7\"\n\nAnd this:\n\nheader-includes:\n  - |\n    \\makeatletter\n    \\renewcommand{\\paragraph}{\\@startsection{paragraph}{4}{\\parindent}%\n      {0\\baselineskip \\@plus 0.2ex \\@minus 0.2ex}%\n      {-1em}%\n      {\\normalfont\\normalsize\\bfseries\\typesectitle}}\n    \n    \\renewcommand{\\subparagraph}[1]{\\@startsection{subparagraph}{5}{1em}%\n      {0\\baselineskip \\@plus 0.2ex \\@minus 0.2ex}%\n      {-\\z@\\relax}%\n      {\\normalfont\\normalsize\\bfseries\\itshape\\hspace{\\parindent}{#1}\\textit{\\addperi}}{\\relax}}\n    \\makeatother\n\nThis was sufficient for my set-up. The additional LaTeX packages I needed were then listed directly under (and in line with) \\makatother."
  },
  {
    "objectID": "posts/papapja7/index.html#testing",
    "href": "posts/papapja7/index.html#testing",
    "title": "APA 7 in papaja",
    "section": "Testing",
    "text": "Testing\nTo test if APA 7 vs. 6 was being used, I changed a BibTex entry for a reference to have more than 7 authors (e.g., by repeating all the author names until &gt;7). I then changed csl: \"``\" to csl: \"``\". In the bibliography only 7 authors were listed (following apa 6). I then changed it back to 'apa7.csl, and the rendered bibliography listed all authors (following apa 7). I took this as sufficient evidence that apa 7 was indeed being used. I then changed the BibTex authors back, of course."
  },
  {
    "objectID": "posts/papapja7/index.html#yaml",
    "href": "posts/papapja7/index.html#yaml",
    "title": "APA 7 in papaja",
    "section": "YAML",
    "text": "YAML\nFor reference, the YAML in my final metafile looks something like this:\n\n---\ntitle             : \"Title\"\nauthor: \n  - name          : \"Daniela Palleschi\"\n\n[...]\n\nbibliography      : [\"references.bib\"]\nfloatsintext      : yes\nfigurelist        : yes\nnumbersections    : yes\ntablelist         : yes\nfootnotelist      : no\nlinenumbers       : yes\nmask              : no\ndraft             : no\ncsl               : \"`r system.file('rmd', 'apa7.csl', package = 'papaja')`\"\ndocumentclass     : \"apa7\"\nclassoption       : \"man\"\nkeep_tex: true\noutput: \n  papaja::apa6_pdf:\n    # citation_package: natbib # so that the TEX file output will contain LaTeX formatted citations\n# natbiboptions: round # so that natbib use parantheses for citations and not square brackets\nbiblio-style: apalike\nauthornote: |\n  \\addORCIDlink{Daniela M. Palleschi}{0000-XXXX-XXXX-XXXX}\nheader-includes:\n- |\n    \\makeatletter\n    \\renewcommand{\\paragraph}{\\@startsection{paragraph}{4}{\\parindent}%\n      {0\\baselineskip \\@plus 0.2ex \\@minus 0.2ex}%\n      {-1em}%\n      {\\normalfont\\normalsize\\bfseries\\typesectitle}}\n    \\renewcommand{\\subparagraph}[1]{\\@startsection{subparagraph}{5}{1em}%\n      {0\\baselineskip \\@plus 0.2ex \\@minus 0.2ex}%\n      {-\\z@\\relax}%\n      {\\normalfont\\normalsize\\bfseries\\itshape\\hspace{\\parindent}{#1}\\textit{\\addperi}}{\\relax}}\n    \\makeatother\n    \\usepackage{float}\n    \\usepackage{multirow}\n    \\usepackage{makecell}\n    \\usepackage{gb4e} \\noautomath\n    \\usepackage{dblfloatfix}\n    \\usepackage{hyperref}\n    \\usepackage{booktabs}\n    \\usepackage{array}\n    \\usepackage{caption}\n    \\captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}\n    \\usepackage{multirow,graphicx}\n    \\usepackage{fdsymbol}\n    \\usepackage{setspace}\n    \\AtBeginEnvironment{tabular}{\\singlespacing}\n    \\AtBeginEnvironment{lltable}{\\singlespacing}\n    \\AtBeginEnvironment{tablenotes}{\\doublespacing}\n    \\captionsetup[table]{font={stretch=1.5}}\n    \\captionsetup[figure]{font={stretch=1.5}}\neditor_options: \n  chunk_output_type: console\n---"
  },
  {
    "objectID": "future-posts/git-with-rproj/index.html",
    "href": "future-posts/git-with-rproj/index.html",
    "title": "GitHub with RProject",
    "section": "",
    "text": "My first blog."
  },
  {
    "objectID": "future-posts/purpose/index.html",
    "href": "future-posts/purpose/index.html",
    "title": "Closing 2023",
    "section": "",
    "text": "It feels strange to be starting a blog dedicated to (lingusitic) data analysis, as like most linguists I’m essentially self-taught in R in that the courses that were offered during my studies did not take me quite as far as I needed to go. As I understand it, this is not something unique to linguistics. As noted in Lawlor et al. (2022) (a highly recommended read), Emery et al. (2021) reported that the vast majority of life science educators surveyed rated data skills as highly important for students, and yet only about half of respondents indicated that such skills were likely to be learned in required or even elective courses, with ‘Course in another department’ receiving the highest overall likelihood rating. I believe we are in a similar situation in the language sciences: For students and researchers to be successful and competitive candidates for future positions/study programs, data skills are important, but they’re not so important that we expect our students will be learning them to satisfaction in our departments? The call is coming from inside the house…\nI say all this as a former linguistics student myself. I see myself in my current students and peers. We started studying linguistics because of the humanities aspect of the field: understanding language, dissecting and developing language theory, observing and describing language patterns and differences between speakers and speaker communities. The last thing we were thinking about was math and computer science, but alas, here I find myself starting a blog on just that topic (or at least a sub-topic). And so, here is one of my intentions for 2024: regularly write mini-tutorials in this blog to (a) help students/peers, (b) map out topics for courses, workshops, or single lectures, and (c) remind future-me how I did something. Though reason (a) might seem totally selfless, when you consider my current position includes teaching and consulting on similar topics, this actually will lighten my workload in the long run, as I won’t have to provide consultations on the same problem over and over again! Reasons (b) and (c) are also pretty self-serving in that they’ll simplify my workflow and productivity. So in conclusion, I’m creating this blog for totally selfish reasons, namely to make my own life easier, and if it helps others in the long run that’s just icing on the cake.\nTo selfishly helping others in 2024, and Auld Lang Syne!\n\n\n\n\n\n\nReferences\n\nEmery, N. C., Crispo, E., Supp, S. R., Farrell, K. J., Kerkhoff, A. J., Bledsoe, E. K., O’Donnell, K. L., McCall, A. C., & Aiello-Lammens, M. E. (2021). Data Science in Undergraduate Life Science Education: A Need for Instructor Skills Training. BioScience, 71(12), 1274–1287. https://doi.org/10.1093/biosci/biab107\n\n\nLawlor, J., Banville, F., Forero-Muñoz, N.-R., Hébert, K., Martínez-Lanfranco, J. A., Rogy, P., & MacDonald, A. A. M. (2022). Ten simple rules for teaching yourself R. PLOS Computational Biology, 18(9), e1010372. https://doi.org/10.1371/journal.pcbi.1010372"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Random forests for variable selection\n\n\n\n\n\n\nrandom forests\n\n\nvariable selection\n\n\nmodel fitting\n\n\n\nDeterming important variables\n\n\n\n\n\nJan 13, 2026\n\n\nDaniela Palleschi\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to random forests\n\n\n\n\n\n\nrandom forests\n\n\ncategorisation\n\n\nmodel selection\n\n\n\nDeterming important variables\n\n\n\n\n\nJan 13, 2026\n\n\nDaniela Palleschi\n\n\n\n\n\n\n\n\n\n\n\n\nHomebrew for Mac\n\n\n\n\n\n\nmac\n\n\npackage management\n\n\n\nUsing Homebrew to update R\n\n\n\n\n\nDec 8, 2025\n\n\nDaniela Palleschi\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating R version\n\n\n\n\n\n\nrenv\n\n\npackage management\n\n\nr version\n\n\n\nUpdating your R version with the renv package\n\n\n\n\n\nDec 8, 2025\n\n\nDaniela Palleschi\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising random effects\n\n\n\n\n\n\nlinear mixed models\n\n\ndata visualisation\n\n\n\nCaterpillar plots with lattice::dotplot() and ggplot2\n\n\n\n\n\nOct 14, 2024\n\n\nDaniela Palleschi\n\n\n\n\n\n\n\n\n\n\n\n\nAPA 7 in papaja\n\n\n\n\n\n\nreproducibility\n\n\nwriting\n\n\n\nSteps to update papaja to APA 7\n\n\n\n\n\nFeb 19, 2024\n\n\nDaniela Palleschi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/homebrew/index.html",
    "href": "posts/homebrew/index.html",
    "title": "Homebrew for Mac",
    "section": "",
    "text": "This blog introduces Homebrew https://brew.sh/, a package manager for Mac users. Homebrew uses the Terminal command line.\n\nInstalling Homebrew\nHomebrew is very simple to install. You just have to enter the following into Terminal and hit enter (if you’ve never used Terminal, just type ‘Terminal’ in the Mac’s search function).\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nYou will likely be prompted to enter your Mac’s login password. The Terminal will not react to your typing (i.e., it doesn’t show the number of characters you type), but just hit enter once you’ve entered the password and it will work (if you’ve entered the password correctly).\nIf successful, you will see the following printed in the Terminal (after a lot of other text):\n==&gt; Installation successful!\n\n==&gt; Homebrew has enabled anonymous aggregate formulae and cask analytics.\nRead the analytics documentation (and how to opt-out) here:\n  https://docs.brew.sh/Analytics\nNo analytics data has been sent yet (nor will any be during this install run).\n\n==&gt; Homebrew is run entirely by unpaid volunteers. Please consider donating:\n  https://github.com/Homebrew/brew#donations\n\n==&gt; Next steps:\n- Run these commands in your terminal to add Homebrew to your PATH:\n    echo &gt;&gt; /Users/danielapalleschi/.zprofile\n    echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; /Users/danielapalleschi/.zprofile\n    eval \"$(/opt/homebrew/bin/brew shellenv)\"\n- Run brew help to get started\n- Further documentation:\n    https://docs.brew.sh\nFollow the instructions for next steps, namely entering the three provided lines sequentially (those starting with echo or eval). Then you should be able to run brew --version to see which Homebrew version you have, and brew help, to see some useful brew commands."
  },
  {
    "objectID": "posts/updater/index.html",
    "href": "posts/updater/index.html",
    "title": "Updating R version",
    "section": "",
    "text": "This blog post discusses the renv package and updating R."
  },
  {
    "objectID": "posts/updater/index.html#installing-packages",
    "href": "posts/updater/index.html#installing-packages",
    "title": "Updating R version",
    "section": "Installing packages",
    "text": "Installing packages\n\nrenv::install()\n\n\nrenv::update()\n\n\nrenv::snapshot()"
  },
  {
    "objectID": "posts/updater/index.html#package-status",
    "href": "posts/updater/index.html#package-status",
    "title": "Updating R version",
    "section": "Package status",
    "text": "Package status\n\nrenv::status()"
  },
  {
    "objectID": "posts/updater/index.html#updating-packages",
    "href": "posts/updater/index.html#updating-packages",
    "title": "Updating R version",
    "section": "Updating packages",
    "text": "Updating packages\n\nrenv::update()"
  },
  {
    "objectID": "posts/updater/index.html#updating-your-lock.file-to-the-new-r-version",
    "href": "posts/updater/index.html#updating-your-lock.file-to-the-new-r-version",
    "title": "Updating R version",
    "section": "Updating your lock.file to the new R version",
    "text": "Updating your lock.file to the new R version\nOpen an R project and navigate to the renv folder. You should see there your previous R version, which is where your projects packages are stored in a lockfile. To transfer these packages over to the new R version, run the following:\n\nR.version # make sure you're on the new R version\n\n\nrenv::status() # check the status; should say there's no renv package\n\n\ninstall.packages(\"renv\") # install renv\n\n\nrenv::status() # should now say there are no packages in the project library\n\n\nrenv::restore() # to install packages in the lockfile from old R version\n\nYou’ll be given three options (activate and use the project library; do not activate the project; cancel). Enter ‘1’ to activate and use the project library.\n\nrenv::update() # update all old packages\n\n\nrenv::snapshot() # save updated lockfile\n\nIf you have broken dependencies, you’ll be given three options (snapshot without fixing any issues, install and/or update packages and then snapshot, or cancel). In this instance I suggest entering ‘2’ to fix any broken dependencies and taking a snapshot. You’ll then be given an overview of the changes that will be made (which will be very long, assuming you have a lot of packages in your lockfile), and asked if you want to proceed. Enter ‘Y’ to do so.\nFinally, check your renv status again.\n\nrenv::status()\n\nYou will hopefully see the following message.\nNo issues found -- the project is in a consistent state."
  },
  {
    "objectID": "posts/random_forests/index.html",
    "href": "posts/random_forests/index.html",
    "title": "Intro to random forests",
    "section": "",
    "text": "Random forests […] work through the data and, by trial and error, establish whether a variable is a useful predictor. The basic algorithm used by the random forests constructs conditional inference trees. A conditional inference tree provides estimates of the likelihood of the value of the response variable (was/were) based on a series of binary questions about the values of predictor variables.\nTagliamonte & Baayen (2012), p.159"
  },
  {
    "objectID": "posts/random_forests/index.html#remove-factors-with-too-many-levels",
    "href": "posts/random_forests/index.html#remove-factors-with-too-many-levels",
    "title": "Intro to random forests",
    "section": "Remove factors with too many levels",
    "text": "Remove factors with too many levels\n\nlibrary(partykit)\nlexdec.cforest &lt;- cforest(\n  RT ~ Class +\n    Correct +\n    Frequency +\n    meanRT +\n    Complex +\n    Length +\n    Sex +\n    Subject +\n    # Word +\n    NativeLanguage,\n  data = lexdec\n)\n\nGet the variance importance measure (varimp(); VIMP) and print them in asending order (sort()). In the partykit package the VIMP is computed via permutation variance importance. Importantly these values are relative, i.e., are only interpretable in comparison to other values from the same random forest. In other words, there is no absolute scale so you can’t compare these values to those from another analysis. Beware (multi)collinearity though, importance can be split across correlated predictors!\n\nlexdec.vimp &lt;- varimp(lexdec.cforest)\nsort(lexdec.vimp)\n\nWe can also visualise these values.\n\nlibrary(lattice)\ndotplot(sort(lexdec.vimp))\n\nHere we see that NativeLanguage and meanRT (which is a participant’s (log) mean reaction time), are highly important relative to the other variables. This is unsurprising, as faster responders will tend to have faster responses, and native English speakers will tend to be faster. These values are relative, so if we remove meanRT, for instance, (since it’s also likely not of theoretical importance), the other values will be affected. We could leave NativeLanguage in, as it might be theoretically interesting to determine whether native speakers have faster lexical decision times."
  },
  {
    "objectID": "posts/random_forests/index.html#remove-uninteresting-but-influential-variable-meanrt",
    "href": "posts/random_forests/index.html#remove-uninteresting-but-influential-variable-meanrt",
    "title": "Intro to random forests",
    "section": "Remove uninteresting (but influential) variable meanRT",
    "text": "Remove uninteresting (but influential) variable meanRT\n\nlexdec.cforest = cforest(\n  RT ~ Class +\n    Correct +\n    Frequency +\n    # meanRT +\n    Complex +\n    Length +\n    Sex +\n    Subject +\n    # Word +\n    NativeLanguage,\n  data = lexdec\n)\n\nWhat have we now got?\n\nlexdec.cforest\n\n500 trees in our forest.\n\nlexdec.vimp &lt;- varimp(lexdec.cforest)\nsort(lexdec.vimp)\n\n\nlibrary(lattice)\ndotplot(sort(lexdec.vimp))\n\nThese seems more interesting from a domain-knowledge perspective: Word frequency is the ‘most important’ variable in that it had a higher contribution (relative to other included predictors) to classification accuracy. This value does not tell us the size or direction of the effect, nor whether the effect would be statistically significant. We do see that participant Sex has a negative value, which indicates that this variable decreases accuracy/is unrelated to our dependent variable and should not be included.\nYou can write:\n\n\n(Permutation-based) variable importance measures indicated that Frequency was the most important predictor, followed by Class, Word…etc.\n\n\nUse these values in addition to your domain-specific knowledge/theoretical justification to select the variables you will include in your model.\nYou can also visualise the variable importance:\n\nlibrary(lattice)\ndotplot(sort(lexdec.vimp))"
  },
  {
    "objectID": "posts/random_forests/index.html#correlation",
    "href": "posts/random_forests/index.html#correlation",
    "title": "Intro to random forests",
    "section": "Correlation",
    "text": "Correlation\nWord length and word frequency are also often correlated. Let’s check this quickly:\n\ncor(lexdec$Frequency,lexdec$Length)\nplot(lexdec$Frequency,lexdec$Length)\n\nThere is a negative correlation (longer words are less frequent). What would happen if we removed word length, since theoretically word frequency might be of more interest here.\n\nlexdec.cforest = cforest(\n  RT ~ Class +\n    Correct +\n    Frequency +\n    # meanRT +\n    # Complex +\n    # Length +\n    # Sex +\n    Subject +\n    # Word +\n    NativeLanguage,\n  data = lexdec\n)\n\n\nlexdec.vimp &lt;- varimp(lexdec.cforest)\nsort(lexdec.vimp)\n\n\ndotplot(sort(lexdec.vimp))\n\nFrequency VIMP increased. The same could be true for whether a word is complex or not.\n\nplot(lexdec$Frequency,lexdec$Complex)\n\nYes, seems less frequent words tended to be coded as 1, which is likely to mean they’re complex.\n\nlexdec.cforest = cforest(\n  RT ~ Class +\n    Correct +\n    Frequency +\n    # meanRT +\n    # Complex +\n    # Length +\n    # Sex +\n    Subject +\n    # Word +\n    NativeLanguage,\n  data = lexdec\n)\n\n\nlexdec.vimp &lt;- varimp(lexdec.cforest)\nsort(lexdec.vimp)\n\n\ndotplot(sort(lexdec.vimp))"
  },
  {
    "objectID": "posts/random_forests-ranger/index.html",
    "href": "posts/random_forests-ranger/index.html",
    "title": "Random forests for variable selection",
    "section": "",
    "text": "Linguistic datasets often contain many potential predictors: frequency, length, surprisal, speaker demographics, etc. This is especially common in fields such as corpus linguistics, sociolinguistics, and phonetics. Deciding which variables actually matter can be a stumbling block when wanting to fit a statistical model to such datasets.\nTraditional approaches such as stepwise regression, p-value filtering, or trial-and-error model comparison are fragile and often misleading, especially when several of these potential predictors are correlated.\nIn this post, we will see how random forests can be used as a robust, prediction-based variable selection tool in R, and how they fit naturally into a theory-driven linguistic workflow. We will also discusss their blindspots and how to be aware of their shortcomings. I would like to emphasise a theory-based method over a data-based method."
  },
  {
    "objectID": "posts/random_forests-ranger/index.html#packages",
    "href": "posts/random_forests-ranger/index.html#packages",
    "title": "Random forests for variable selection",
    "section": "Packages",
    "text": "Packages\nNext let’s install our required packages.\nWe’ll need the following packages.\n\nlibrary(tidyverse)\nlibrary(ranger)\nlibrary(vip)\nlibrary(lme4)\nlibrary(MASS)    # For mvrnorm"
  },
  {
    "objectID": "posts/random_forests-ranger/index.html#simulate-data",
    "href": "posts/random_forests-ranger/index.html#simulate-data",
    "title": "Random forests for variable selection",
    "section": "Simulate data",
    "text": "Simulate data\nWe start by setting a random number generator for reproducibility purposes. We then generate a dataset with the following variables:\n\nfrequency\nlength\nanimacy\ndefiniteness\nsurprisal\nspeaker\n\nThen, we create a\n\nset.seed(123)\nn &lt;- 2000\nn_speakers &lt;- 40\n\n# Correlation between log-frequency and length (negative: short words are more frequent)\nmu &lt;- c(log_freq = 2, length = 5)\nSigma &lt;- matrix(c(1, -0.6, -0.6, 2), 2, 2)  # cov matrix: negative correlation\n\ncorrelated_predictors &lt;- mvrnorm(n, mu = mu, Sigma = Sigma) %&gt;%\n  as_tibble() %&gt;%\n  rename(log_frequency = log_freq) %&gt;%\n  mutate(frequency = exp(log_frequency),\n         length = round(length))\n\n# Speaker random effects\nspeaker_effects &lt;- rnorm(n_speakers, 0, 30)\n\ndata &lt;- correlated_predictors %&gt;%\n  mutate(\n    animacy      = sample(c(\"animate\", \"inanimate\"), n, TRUE),\n    definiteness = sample(c(\"def\", \"indef\"), n, TRUE),\n    surprisal    = rnorm(n, 5, 1),\n    speaker      = sample(1:n_speakers, n, TRUE),\n    speaker_effect = speaker_effects[speaker],\n    \n    # Continuous outcome: reaction time (RT)\n    rt = 500 - 30 * log_frequency + 10 * length - if_else(animacy == \"animate\", 20, 0) +\n         speaker_effect + rnorm(n, 0, 50),\n    \n    # Categorical outcome: syntactic choice\n    response = if_else(\n      0.8 * log_frequency - 0.5 * length + if_else(animacy == \"animate\", 0.7, 0) +\n      speaker_effect/50 + rnorm(n, 0, 0.5) &gt; 0,\n      \"A\", \"B\"\n    ) %&gt;% factor()\n  )"
  },
  {
    "objectID": "posts/random_forests-ranger/index.html#check-collinearity",
    "href": "posts/random_forests-ranger/index.html#check-collinearity",
    "title": "Random forests for variable selection",
    "section": "Check collinearity",
    "text": "Check collinearity\nWhen predictors are correlated, regression coefficients can become unstable, standard errors inflate, and it becomes tricky to interpret individual effects. This is because regression assumes predictors are not correlated\n\nlibrary(car)\n\nvif(lmer_rt_slope)"
  },
  {
    "objectID": "posts/random_forests-ranger/index.html#remove-factors-with-too-many-levels",
    "href": "posts/random_forests-ranger/index.html#remove-factors-with-too-many-levels",
    "title": "Random forests for variable selection",
    "section": "Remove factors with too many levels",
    "text": "Remove factors with too many levels\n\ncforest_lexdec &lt;- cforest(\n  RT ~ Class +\n    Correct +\n    Frequency +\n    meanRT +\n    Complex +\n    Length +\n    Sex +\n    Subject +\n    # Word +\n    NativeLanguage,\n  data = df_lexdec\n)\n\nWhat does our forest contain?\n\nsummary(cforest_lexdec)\n\nTo check the dependent variable used for a particular forest:\n\ncforest_lexdec$terms[[2]]\n\nAnd the predictors:\n\ncforest_lexdec$terms[[3]]\n\nGet the variance importance measure (varimp(); VIMP) and print them in asending order (sort()).\n\nvimp_lexdec &lt;- varimp(cforest_lexdec)\nsort(vimp_lexdec)\n\nWe can also visualise these values with the dotplot function from the lattice package.\n\nlibrary(lattice)\ndotplot(sort(vimp_lexdec))\n\n\n\n\n\n\n\nVariance importance\n\n\n\nIn the partykit package the VIMP is computed via permutation variance importance. Importantly these values are relative, i.e., are only interpretable in comparison to other values from the same random forest. In other words, there is no absolute scale so you can’t compare these values to those from another analysis. Beware (multi)collinearity though, importance can be split across correlated predictors!\n\n\nHere we see that NativeLanguage and meanRT (which is a participant’s (log) mean reaction time), are highly important relative to the other variables. This is unsurprising, as faster responders will tend to have faster responses, and native English speakers will tend to be faster. These values are relative, so if we remove meanRT, for instance, (since it’s also likely not of theoretical importance), the other values will be affected. We could leave NativeLanguage in, as it might be theoretically interesting to determine whether native speakers have faster lexical decision times."
  },
  {
    "objectID": "posts/random_forests-ranger/index.html#remove-uninteresting-but-influential-variable-meanrt",
    "href": "posts/random_forests-ranger/index.html#remove-uninteresting-but-influential-variable-meanrt",
    "title": "Random forests for variable selection",
    "section": "Remove uninteresting (but influential) variable meanRT",
    "text": "Remove uninteresting (but influential) variable meanRT\n\ncforest_lexdec = cforest(\n  RT ~ Class +\n    Correct +\n    Frequency +\n    # meanRT +\n    Complex +\n    Length +\n    Sex +\n    Subject +\n    # Word +\n    NativeLanguage,\n  data = df_lexdec\n)\n\nWhat have we now got?\n\nsummary(cforest_lexdec)\n\n500 trees in our forest.\n\nvimp_lexdec &lt;- varimp(cforest_lexdec)\nsort(vimp_lexdec)\n\n\ndotplot(sort(vimp_lexdec))\n\n\nplot(cforest_lexdec[[1]])\n\nThese seems more interesting from a domain-knowledge perspective: Looking at the factors relevant to the stimuli (the word), rather than the participants: Word frequency is the ‘most important’ variable in that it had a higher contribution (relative to other included predictors) to classification accuracy. This value does not tell us the size or direction of the effect, nor whether the effect would be statistically significant.\nTurning to the participant-relevant factors of Subject (which subject the reaction time came from) and NativeLanguage (whether the participant is a native speaker of English or not), these two are the most important. This means that the knowing which participant the data point comes from is very important for predicting the value of the reaction time, which is typical in linguistic research as inter-individual differences tend to account for a large amount of variability. This is why mixed-effects models are highly important in most linguistic research, as data points are not independent. Next is the question of NativeLanguage, which also has high importance unsurprisingly. Individuals will tend to be faster at making lexical decisions in their native language than in a non-native language.\nYou can write:\n\n\n(Permutation-based) variable importance measures indicated that Frequency was the most important predictor, followed by Class, Word…etc.\n\n\nUse these values in addition to your domain-specific knowledge/theoretical justification to select the variables you will include in your model.\nYou can also visualise the variable importance:\n\nlibrary(lattice)\ndotplot(sort(vimp_lexdec))"
  },
  {
    "objectID": "posts/random_forests-ranger/index.html#colinearity",
    "href": "posts/random_forests-ranger/index.html#colinearity",
    "title": "Random forests for variable selection",
    "section": "Colinearity",
    "text": "Colinearity\nWord length and word frequency are also often correlated. Let’s check this quickly:\n\ncor(df_lexdec$Frequency,df_lexdec$Length)\nplot(df_lexdec$Frequency,df_lexdec$Length)\n\nThere is a negative correlation (longer words are less frequent). What would happen if we removed word length, since theoretically word frequency might be of more interest here.\n\ncforest_lexdec = cforest(\n  RT ~ Class +\n    Correct +\n    Frequency +\n    # meanRT +\n    # Complex +\n    # Length +\n    # Sex +\n    Subject +\n    # Word +\n    NativeLanguage,\n  data = df_lexdec\n)\n\n\nvimp_lexdec &lt;- varimp(cforest_lexdec)\nsort(vimp_lexdec)\n\n\ndotplot(sort(vimp_lexdec))\n\nFrequency VIMP increased. The same could be true for whether a word is complex or not.\n\nplot(df_lexdec$Frequency,df_lexdec$Complex)\n\nYes, seems less frequent words tended to be coded as 1, which is likely to mean they’re complex.\n\ncforest_lexdec = cforest(\n  RT ~ Class +\n    Correct +\n    Frequency +\n    # meanRT +\n    # Complex +\n    # Length +\n    # Sex +\n    Subject +\n    # Word +\n    NativeLanguage,\n  data = df_lexdec\n)\n\n\nvimp_lexdec &lt;- varimp(cforest_lexdec)\nsort(vimp_lexdec)\n\n\ndotplot(sort(vimp_lexdec))"
  }
]
[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is run by Daniela Palleschi, a researcher and lecturer at the Institute of German Language and Linguistics (Institut für deutsche Sprache und Linguistik) at the Humboldt-Universität zu Berlin. Her current position includes teaching statistics and quantitative methods, as well as providing consultations on these topics to students.\nBlog posts are typically written with students in mind, with the aim of addressing common problems that they seem to have."
  },
  {
    "objectID": "future-posts/my-post/index.html",
    "href": "future-posts/my-post/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "future-posts/repro/index.html",
    "href": "future-posts/repro/index.html",
    "title": "Reproducibility 101",
    "section": "",
    "text": "Bryan, Jenny, and Jim Hester. What They Forgot to Teach You About R. (2019). url: https://rstats.wtf/\nBryan, Jenny (). Happy Git and GitHub for the useRhttps://happygitwithr.com/\nBlogpost An Introduction to Docker for R Users by Colin Fay https://colinfay.me/docker-r-reproducibility/\nReproducible Work in R: Why and How to use Docker in your daily R workflow (2019) by Rahul S. https://towardsdatascience.com/reproducible-work-in-r-e7d160d5d198\n\n\n\n\n[here]\n[renv]\n[targets]\n[docker]"
  },
  {
    "objectID": "future-posts/repro/index.html#reading",
    "href": "future-posts/repro/index.html#reading",
    "title": "Reproducibility 101",
    "section": "",
    "text": "Bryan, Jenny, and Jim Hester. What They Forgot to Teach You About R. (2019). url: https://rstats.wtf/\nBryan, Jenny (). Happy Git and GitHub for the useRhttps://happygitwithr.com/\nBlogpost An Introduction to Docker for R Users by Colin Fay https://colinfay.me/docker-r-reproducibility/\nReproducible Work in R: Why and How to use Docker in your daily R workflow (2019) by Rahul S. https://towardsdatascience.com/reproducible-work-in-r-e7d160d5d198"
  },
  {
    "objectID": "future-posts/repro/index.html#packages",
    "href": "future-posts/repro/index.html#packages",
    "title": "Reproducibility 101",
    "section": "",
    "text": "[here]\n[renv]\n[targets]\n[docker]"
  },
  {
    "objectID": "future-posts/ggeffects/index.html",
    "href": "future-posts/ggeffects/index.html",
    "title": "ggeffects Package",
    "section": "",
    "text": "When reporting our models, it can be helpful for our audience (and ourselves) to produce model predicted values per condition/values of a continuous predictor. Using the equation of a line, we can manually generate such predictions as long as we know:\nThere are also useful packages that automate this process for us, which can be especially useful with more complex models such as mixed effects models.\nWe’ll see here how to manually compute the fitted values per condition, and how these compare to functions from the ggeffects package."
  },
  {
    "objectID": "future-posts/ggeffects/index.html#fit-model",
    "href": "future-posts/ggeffects/index.html#fit-model",
    "title": "ggeffects Package",
    "section": "Fit model",
    "text": "Fit model\n\nglmer_dative &lt;- \n  glmer(\n  realization_of_recipient ~ semantic_class + length_of_recipient +\n    (1 | verb),\n  # control = glmerControl(optimizer = \"optimx\", optCtrl = list(method = \"nlminb\")),\n  data = df_dative,\n  family = \"binomial\"\n)\n\n\nsummary(glmer_dative)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: realization_of_recipient ~ semantic_class + length_of_recipient +  \n    (1 | verb)\n   Data: df_dative\n\n     AIC      BIC   logLik deviance df.resid \n  1710.4   1750.7   -848.2   1696.4     2353 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-33.155  -0.443  -0.272  -0.042   5.764 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n verb   (Intercept) 1.715    1.31    \nNumber of obs: 2360, groups:  verb, 38\n\nFixed effects:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -3.05370    0.35904  -8.505   &lt;2e-16 ***\nsemantic_classc      0.07234    0.31981   0.226   0.8210    \nsemantic_classf      0.53767    0.66759   0.805   0.4206    \nsemantic_classp     -3.68208    1.43821  -2.560   0.0105 *  \nsemantic_classt      0.97386    0.16237   5.998    2e-09 ***\nlength_of_recipient  0.98917    0.08216  12.039   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) smntc_clssc smntc_clssf smntc_clssp smntc_clsst\nsmntc_clssc -0.307                                                \nsmntc_clssf -0.280  0.098                                         \nsmntc_clssp -0.185  0.055       0.050                             \nsmntc_clsst -0.253  0.261       0.054       0.070                 \nlngth_f_rcp -0.391  0.077       0.021       0.011       0.085"
  },
  {
    "objectID": "future-posts/ggeffects/index.html#calculate-predictions",
    "href": "future-posts/ggeffects/index.html#calculate-predictions",
    "title": "ggeffects Package",
    "section": "Calculate predictions",
    "text": "Calculate predictions\nFirst, remind ourselves of our contrasts\n\ncontrasts(df_dative$semantic_class)\n\n  c f p t\na 0 0 0 0\nc 1 0 0 0\nf 0 1 0 0\np 0 0 1 0\nt 0 0 0 1\n\n\nGrab our estimates.\n\ntidy(glmer_dative)\n\n# A tibble: 7 × 7\n  effect   group term                estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;  (Intercept)          -3.05      0.359     -8.51   1.81e-17\n2 fixed    &lt;NA&gt;  semantic_classc       0.0723    0.320      0.226  8.21e- 1\n3 fixed    &lt;NA&gt;  semantic_classf       0.538     0.668      0.805  4.21e- 1\n4 fixed    &lt;NA&gt;  semantic_classp      -3.68      1.44      -2.56   1.05e- 2\n5 fixed    &lt;NA&gt;  semantic_classt       0.974     0.162      6.00   2.00e- 9\n6 fixed    &lt;NA&gt;  length_of_recipient   0.989     0.0822    12.0    2.22e-33\n7 ran_pars verb  sd__(Intercept)       1.31     NA         NA     NA       \n\n\nSave them as objects to make things a little more transparent.\n\nintercept &lt;-\n  tidy(glmer_dative) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nsem_class_a_c &lt;-\n  tidy(glmer_dative) |&gt; \n  filter(term == \"semantic_classc\") |&gt; \n  pull(estimate)\n\nsem_class_a_f &lt;-\n  tidy(glmer_dative) |&gt; \n  filter(term == \"semantic_classf\") |&gt; \n  pull(estimate)\n\nsem_class_a_p &lt;-\n  tidy(glmer_dative) |&gt; \n  filter(term == \"semantic_classp\") |&gt; \n  pull(estimate)\n\nsem_class_a_t &lt;-\n  tidy(glmer_dative) |&gt; \n  filter(term == \"semantic_classt\") |&gt; \n  pull(estimate)\n\nlength_recip &lt;- \n  tidy(glmer_dative) |&gt; \n  filter(term == \"length_of_recipient\") |&gt; \n  pull(estimate)\n\nNow we can manually calculate:\n\na &lt;- plogis(intercept + sem_class_a_c*0 + length_recip*1)\n\n\nc &lt;- plogis(intercept + sem_class_a_c*1 + length_recip*1)\n\n\nf &lt;- plogis(intercept + sem_class_a_f*1 + length_recip*1)\n\n\np &lt;- plogis(intercept + sem_class_a_p*1 + length_recip*1)\n\n\nt &lt;- plogis(intercept + sem_class_a_t*1 + length_recip*1)\n\n\na\n\n[1] 0.1125924\n\nc\n\n[1] 0.1200255\n\nf\n\n[1] 0.1784539\n\np\n\n[1] 0.003183409\n\nt\n\n[1] 0.2514916\n\n\n\nggeffects::ggpredict(glmer_dative, terms = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.11 | 0.06, 0.19\nc              |      0.12 | 0.06, 0.22\nf              |      0.18 | 0.06, 0.43\np              |      0.00 | 0.00, 0.05\nt              |      0.25 | 0.16, 0.38\n\nAdjusted for:\n* length_of_recipient = 1.00\n*                verb = 0 (population-level)\n\n\nThis matches our manual calculations.\n\nggeffects::ggeffect(glmer_dative, terms = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.16 | 0.09, 0.26\nc              |      0.17 | 0.09, 0.30\nf              |      0.24 | 0.08, 0.53\np              |      0.00 | 0.00, 0.07\nt              |      0.33 | 0.20, 0.49\n\n\nThis doesn’t…why is that?\n\nggeffects::ggemmeans(glmer_dative, terms = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.11 | 0.06, 0.20\nc              |      0.12 | 0.06, 0.22\nf              |      0.18 | 0.06, 0.44\np              |      0.00 | 0.00, 0.05\nt              |      0.25 | 0.15, 0.39\n\nAdjusted for:\n* length_of_recipient = 1.00\n\n\nThis also matches our predictions."
  },
  {
    "objectID": "future-posts/ggeffects/index.html#centre-predictors",
    "href": "future-posts/ggeffects/index.html#centre-predictors",
    "title": "ggeffects Package",
    "section": "Centre predictors",
    "text": "Centre predictors\n\ncontrasts(df_dative$semantic_class)\n\n  c f p t\na 0 0 0 0\nc 1 0 0 0\nf 0 1 0 0\np 0 0 1 0\nt 0 0 0 1\n\ncontrasts(df_dative$semantic_class) &lt;- contr.sum(5)/2\ncontrasts(df_dative$semantic_class)\n\n  [,1] [,2] [,3] [,4]\na  0.5  0.0  0.0  0.0\nc  0.0  0.5  0.0  0.0\nf  0.0  0.0  0.5  0.0\np  0.0  0.0  0.0  0.5\nt -0.5 -0.5 -0.5 -0.5\n\n\n\nsummary(df_dative$length_of_recipient)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.386   1.000  15.000 \n\nhist(df_dative$length_of_recipient)\n\n\n\n\n\n\n\n\n\ndf_dative &lt;-\n  df_dative |&gt; \n  mutate(length_of_recipient_z = scale(df_dative$length_of_recipient))\n\n\ndf_dative |&gt; \n  select(length_of_recipient, length_of_recipient_z) |&gt; \n  head()\n\n    length_of_recipient length_of_recipient_z\n903                   1            -0.3283482\n904                   2             0.5213247\n905                   1            -0.3283482\n906                   2             0.5213247\n907                   2             0.5213247\n908                   1            -0.3283482"
  },
  {
    "objectID": "future-posts/ggeffects/index.html#fit-model-1",
    "href": "future-posts/ggeffects/index.html#fit-model-1",
    "title": "ggeffects Package",
    "section": "Fit model",
    "text": "Fit model\n\nglmer_dative_c &lt;- \n  glmer(\n  realization_of_recipient ~ semantic_class + length_of_recipient +\n    (1 | verb),\n  # control = glmerControl(optimizer = \"optimx\", optCtrl = list(method = \"nlminb\")),\n  data = df_dative,\n  family = \"binomial\"\n)\n\n\nsummary(glmer_dative_c)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: realization_of_recipient ~ semantic_class + length_of_recipient +  \n    (1 | verb)\n   Data: df_dative\n\n     AIC      BIC   logLik deviance df.resid \n  1710.4   1750.7   -848.2   1696.4     2353 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-33.155  -0.443  -0.272  -0.042   5.764 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n verb   (Intercept) 1.715    1.31    \nNumber of obs: 2360, groups:  verb, 38\n\nFixed effects:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         -3.47333    0.39977  -8.688  &lt; 2e-16 ***\nsemantic_class1      0.83925    0.68060   1.233  0.21754    \nsemantic_class2      0.98392    0.78282   1.257  0.20880    \nsemantic_class3      1.91463    1.18820   1.611  0.10710    \nsemantic_class4     -6.52475    2.29360  -2.845  0.00444 ** \nlength_of_recipient  0.98917    0.08216  12.039  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) smnt_1 smnt_2 smnt_3 smnt_4\nsmntc_clss1 -0.539                            \nsmntc_clss2 -0.486  0.626                     \nsmntc_clss3 -0.152  0.057  0.000              \nsmntc_clss4  0.570 -0.805 -0.715 -0.543       \nlngth_f_rcp -0.317 -0.040  0.028  0.001  0.002\n\n\n\ntidy(glmer_dative_c)\n\n# A tibble: 7 × 7\n  effect   group term                estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;  (Intercept)           -3.47     0.400      -8.69  3.68e-18\n2 fixed    &lt;NA&gt;  semantic_class1        0.839    0.681       1.23  2.18e- 1\n3 fixed    &lt;NA&gt;  semantic_class2        0.984    0.783       1.26  2.09e- 1\n4 fixed    &lt;NA&gt;  semantic_class3        1.91     1.19        1.61  1.07e- 1\n5 fixed    &lt;NA&gt;  semantic_class4       -6.52     2.29       -2.84  4.44e- 3\n6 fixed    &lt;NA&gt;  length_of_recipient    0.989    0.0822     12.0   2.21e-33\n7 ran_pars verb  sd__(Intercept)        1.31    NA          NA    NA       \n\n\nOur estimates have changed.\nLet’s first take a look at the predictions computed by the ggeffects package.\n\nggpredict(glmer_dative_c, term = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.11 | 0.06, 0.19\nc              |      0.12 | 0.06, 0.22\nf              |      0.18 | 0.06, 0.43\np              |      0.00 | 0.00, 0.05\nt              |      0.25 | 0.16, 0.38\n\nAdjusted for:\n* length_of_recipient = 1.00\n*                verb = 0 (population-level)\n\n\nThese are the same as before.\n\nggeffect(glmer_dative_c, term = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.16 | 0.09, 0.26\nc              |      0.17 | 0.09, 0.30\nf              |      0.24 | 0.08, 0.53\np              |      0.00 | 0.00, 0.07\nt              |      0.33 | 0.20, 0.49\n\n\nThese are also the same as before.\n\nggemmeans(glmer_dative_c, term = \"semantic_class\")\n\n# Predicted probabilities of realization_of_recipient\n\nsemantic_class | Predicted |     95% CI\n---------------------------------------\na              |      0.11 | 0.06, 0.20\nc              |      0.12 | 0.06, 0.22\nf              |      0.18 | 0.06, 0.44\np              |      0.00 | 0.00, 0.05\nt              |      0.25 | 0.15, 0.39\n\nAdjusted for:\n* length_of_recipient = 1.00\n\n\nThese are again the same as before, and match those computed by ggpredict().\nSo what’s going on here? To understand it better, let’s first repeat the manual calculations using the model estimates."
  },
  {
    "objectID": "future-posts/ggeffects/index.html#manual-predictions",
    "href": "future-posts/ggeffects/index.html#manual-predictions",
    "title": "ggeffects Package",
    "section": "Manual predictions",
    "text": "Manual predictions\nFirst, remind ourselves of our contrasts\n\ncontrasts(df_dative$semantic_class)\n\n  [,1] [,2] [,3] [,4]\na  0.5  0.0  0.0  0.0\nc  0.0  0.5  0.0  0.0\nf  0.0  0.0  0.5  0.0\np  0.0  0.0  0.0  0.5\nt -0.5 -0.5 -0.5 -0.5\n\n\nGrab our estimates.\n\ntidy(glmer_dative_c)\n\n# A tibble: 7 × 7\n  effect   group term                estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;  (Intercept)           -3.47     0.400      -8.69  3.68e-18\n2 fixed    &lt;NA&gt;  semantic_class1        0.839    0.681       1.23  2.18e- 1\n3 fixed    &lt;NA&gt;  semantic_class2        0.984    0.783       1.26  2.09e- 1\n4 fixed    &lt;NA&gt;  semantic_class3        1.91     1.19        1.61  1.07e- 1\n5 fixed    &lt;NA&gt;  semantic_class4       -6.52     2.29       -2.84  4.44e- 3\n6 fixed    &lt;NA&gt;  length_of_recipient    0.989    0.0822     12.0   2.21e-33\n7 ran_pars verb  sd__(Intercept)        1.31    NA          NA    NA       \n\n\nSave them as objects to make things a little more transparent.\n\nintercept &lt;-\n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nsem_class_t_a &lt;-\n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"semantic_class1\") |&gt; \n  pull(estimate)\n\nsem_class_t_c &lt;-\n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"semantic_class2\") |&gt; \n  pull(estimate)\n\nsem_class_t_f &lt;-\n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"semantic_class3\") |&gt; \n  pull(estimate)\n\nsem_class_t_p &lt;-\n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"semantic_class4\") |&gt; \n  pull(estimate)\n\nlength_recip &lt;- \n  tidy(glmer_dative_c) |&gt; \n  filter(term == \"length_of_recipient\") |&gt; \n  pull(estimate)\n\nNow we can manually calculate:\n\n# t &lt;- \n  plogis(intercept + sem_class_t_a*(-0.5))\n\n[1] 0.01997777\n\nplogis(intercept + sem_class_t_p*(-0.5))\n\n[1] 0.4474558\n\nplogis(intercept + sem_class_t_f*(-0.5))\n\n[1] 0.01176666\n\nplogis(intercept + sem_class_t_c*(-0.5))\n\n[1] 0.01860966\n\n\n\nsummary(df_dative$semantic_class)\n\n  a   c   f   p   t \n907 371  47 183 852 \n\n\n\na &lt;- plogis(intercept + sem_class_t_a*(+0.5) + length_recip*0)\n\n\nf &lt;- plogis(intercept + sem_class_t_f*(+0.5) + length_recip*0)\n\n\np &lt;- plogis(intercept + sem_class_t_p*(+0.5) + length_recip*0)\n\n\nc &lt;- plogis(intercept + sem_class_t_c*(+0.5) + length_recip*0)\n\n\na\n\n[1] 0.04505766\n\nc\n\n[1] 0.04827448\n\nf\n\n[1] 0.07474313\n\np\n\n[1] 0.001186326\n\nt\n\n[1] 0.2514916"
  },
  {
    "objectID": "future-posts/ggeffects/index.html#difference-between-ggeffect-ggpredict-ggemmeans",
    "href": "future-posts/ggeffects/index.html#difference-between-ggeffect-ggpredict-ggemmeans",
    "title": "ggeffects Package",
    "section": "Difference between ggeffect(), ggpredict(), ggemmeans()",
    "text": "Difference between ggeffect(), ggpredict(), ggemmeans()"
  },
  {
    "objectID": "posts/dotplot/index.html",
    "href": "posts/dotplot/index.html",
    "title": "Visualising random effects",
    "section": "",
    "text": "This blog post goes over how to produce visualisations of by-grouping factor (e.g., by-participant) varying intercepts and slopes.\nRequired packages for reproducible example:\n## install 'pacman' if you don't have it\n# install.packages(\"pacman\")\n\n# p_load(): load or install all listed CRAN packages\npacman::p_load(\n  languageR, # for the example data\n  dplyr, # summarising our data\n  janitor, # for summarising our data\n  lme4, # fitting our model\n  lattice # for the caterpillar plots\n)"
  },
  {
    "objectID": "posts/dotplot/index.html#load-repeated-measures-data",
    "href": "posts/dotplot/index.html#load-repeated-measures-data",
    "title": "Visualising random effects",
    "section": "Load repeated measures data",
    "text": "Load repeated measures data\nWe’ll use some toy data.\n\ndf_lexdec &lt;- languageR::lexdec\n\nCheck out the variable names:\n\nnames(df_lexdec)\n\n [1] \"Subject\"        \"RT\"             \"Trial\"          \"Sex\"           \n [5] \"NativeLanguage\" \"Correct\"        \"PrevType\"       \"PrevCorrect\"   \n [9] \"Word\"           \"Frequency\"      \"FamilySize\"     \"SynsetCount\"   \n[13] \"Length\"         \"Class\"          \"FreqSingular\"   \"FreqPlural\"    \n[17] \"DerivEntropy\"   \"Complex\"        \"rInfl\"          \"meanRT\"        \n[21] \"SubjFreq\"       \"meanSize\"       \"meanWeight\"     \"BNCw\"          \n[25] \"BNCc\"           \"BNCd\"           \"BNCcRatio\"      \"BNCdRatio\"     \n\n\nLet’s look at the first few rows of some selected columns:\n\nhead(df_lexdec[c(1:5,9)])\n\n  Subject       RT Trial Sex NativeLanguage       Word\n1      A1 6.340359    23   F        English        owl\n2      A1 6.308098    27   F        English       mole\n3      A1 6.349139    29   F        English     cherry\n4      A1 6.186209    30   F        English       pear\n5      A1 6.025866    32   F        English        dog\n6      A1 6.180017    33   F        English blackberry\n\n\nHow many participants?\n\nlength(unique(df_lexdec$Subject))\n\n[1] 21\n\n\nThere are 21 participants in this dataset. How many unique words were there?\n\nlength(unique(df_lexdec$Word))\n\n[1] 79\n\n\nThere are 79 words.\nHow many observations (rows) per participant?\n\ndf_lexdec |&gt; \n  dplyr::count(Subject) |&gt; \n  count(n, name = \"Subject\")\n\n   n Subject\n1 79      21\n\n\nWe see that all 21 participants had 79 observations, i.e., this was a repeated measures design. This is nice and clean, as 79 x 21 equals the number of observations in the dataset (1659):\n\n79*21 == nrow(df_lexdec)\n\n[1] TRUE\n\n\nThis means there are no missing values. Wouldn’t have been a problem for our model, but is good to know (and always good to double check)."
  },
  {
    "objectID": "posts/dotplot/index.html#mixed-effects-model",
    "href": "posts/dotplot/index.html#mixed-effects-model",
    "title": "Visualising random effects",
    "section": "Mixed effects model",
    "text": "Mixed effects model\nWe’ll ignore a lot of steps for the sake of simplicity here. Let’s fit a linear mixed model with log reaction times (RT) as predicted by word frequency (Frequency), with by-participant (Subject) and by-word (Word) varying intercepts and slopes.\n\nmod_lexdec &lt;-\n  lmer(RT ~ Frequency + NativeLanguage +\n         (1 + Frequency | Subject) +\n         (1 + NativeLanguage | Word),\n       data = df_lexdec,\n       control = lmerControl(optimizer = \"bobyqa\")\n       )\n\nFirst: note we have a single varying slope for participant and word each. The effect of Frequency is allowed to vary between participants because each participant contributed data points for words of different frequencies (presumably). The effect of NativeLanguage is allowed to vary between words because we have data points from different NativeLanguage levels (English, Other) for each word (presumably). Importantly, NativeLanguage does not vary within a single subject, nor does word frequency vary within a single word.\n\nInspect model\n\nsummary(mod_lexdec)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Frequency + NativeLanguage + (1 + Frequency | Subject) +  \n    (1 + NativeLanguage | Word)\n   Data: df_lexdec\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: -964.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.3977 -0.6132 -0.1193  0.4740  6.2917 \n\nRandom effects:\n Groups   Name                Variance  Std.Dev. Corr \n Word     (Intercept)         0.0017172 0.04144       \n          NativeLanguageOther 0.0014730 0.03838  0.74 \n Subject  (Intercept)         0.0462392 0.21503       \n          Frequency           0.0003824 0.01956  -0.88\n Residual                     0.0288063 0.16972       \nNumber of obs: 1659, groups:  Word, 79; Subject, 21\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)          6.540876   0.060575 107.981\nFrequency           -0.039879   0.007122  -5.599\nNativeLanguageOther  0.078591   0.054036   1.454\n\nCorrelation of Fixed Effects:\n            (Intr) Frqncy\nFrequency   -0.805       \nNtvLnggOthr -0.433  0.103\n\n\nIn terms of fixed effects, we see a negative slope for Frequency, indicating shorter log reaction times for more frequent words. We see a positive slope for NativeLanguageOther, indicating longer log reaction times for participants whose first language was not English. We will not consider the size of these effects nor whether they’re “significant”."
  },
  {
    "objectID": "posts/dotplot/index.html#random-effects",
    "href": "posts/dotplot/index.html#random-effects",
    "title": "Visualising random effects",
    "section": "Random effects",
    "text": "Random effects\nWe can inspect just the random effects and the variance-covariance matrix:\n\nVarCorr(mod_lexdec)\n\n Groups   Name                Std.Dev. Corr  \n Word     (Intercept)         0.041439       \n          NativeLanguageOther 0.038380 0.742 \n Subject  (Intercept)         0.215033       \n          Frequency           0.019556 -0.877\n Residual                     0.169724       \n\n\nWe can get the divergence of each level of our grouping factors from the population-level estimates:\n\nranef(mod_lexdec)$Subject\n\n   (Intercept)     Frequency\nA1 -0.06030431 -0.0022251255\nA2 -0.18962004  0.0128083423\nA3 -0.06664595  0.0072151844\nC  -0.09377251  0.0133606863\nD  -0.06141404  0.0077909999\nI  -0.18734788  0.0033222900\nJ  -0.25375499  0.0171447631\nK  -0.22563195  0.0147162271\nM1 -0.29229275  0.0244231510\nM2  0.25110977 -0.0308968328\nP   0.08034419 -0.0165141417\nR1 -0.14261330  0.0203393705\nR2  0.12464130 -0.0015903453\nR3  0.04157428 -0.0002952847\nS   0.03561165 -0.0028669061\nT1  0.02619380 -0.0030887838\nT2  0.52058557 -0.0316672952\nV   0.17933015 -0.0055399270\nW1 -0.13602827  0.0104647912\nW2  0.12954541 -0.0049044182\nZ   0.32048987 -0.0319967457\n\n\n\nhead(ranef(mod_lexdec)$Word, n = 10)\n\n           (Intercept) NativeLanguageOther\nalmond     0.018780910         0.026225728\nant       -0.022568074        -0.024627285\napple     -0.040907427        -0.035793135\napricot   -0.032899686        -0.033362009\nasparagus  0.023935099         0.013473767\navocado   -0.017600540        -0.014486640\nbanana    -0.036796340        -0.046064766\nbat       -0.016603440        -0.015256600\nbeaver     0.003163019         0.005906525\nbee       -0.026069500        -0.023041857\n\n\nI’ve printed just the first 10 words to avoid printing all 79. Notice the values consist of a mix of positive and negative values. This is because the population-level estimate (i.e., the fixed effects) lies in the middle of all the observations. These are the deviances, i.e., how each participant’s or word’s fitted intercept or slope value deviates from the population-level value.\nWe can also print the actual fitted values:\n\ncoef(mod_lexdec)$Subject\n\n   (Intercept)   Frequency NativeLanguageOther\nA1    6.480572 -0.04210381          0.07859106\nA2    6.351256 -0.02707035          0.07859106\nA3    6.474230 -0.03266350          0.07859106\nC     6.447103 -0.02651800          0.07859106\nD     6.479462 -0.03208769          0.07859106\nI     6.353528 -0.03655640          0.07859106\nJ     6.287121 -0.02273392          0.07859106\nK     6.315244 -0.02516246          0.07859106\nM1    6.248583 -0.01545554          0.07859106\nM2    6.791986 -0.07077552          0.07859106\nP     6.621220 -0.05639283          0.07859106\nR1    6.398263 -0.01953932          0.07859106\nR2    6.665517 -0.04146903          0.07859106\nR3    6.582450 -0.04017397          0.07859106\nS     6.576488 -0.04274559          0.07859106\nT1    6.567070 -0.04296747          0.07859106\nT2    7.061461 -0.07154598          0.07859106\nV     6.720206 -0.04541861          0.07859106\nW1    6.404848 -0.02941390          0.07859106\nW2    6.670421 -0.04478311          0.07859106\nZ     6.861366 -0.07187543          0.07859106\n\n\n\nhead(coef(mod_lexdec)$Word, n = 10)\n\n          (Intercept)   Frequency NativeLanguageOther\nalmond       6.559657 -0.03987869          0.10481679\nant          6.518308 -0.03987869          0.05396378\napple        6.499968 -0.03987869          0.04279793\napricot      6.507976 -0.03987869          0.04522905\nasparagus    6.564811 -0.03987869          0.09206483\navocado      6.523275 -0.03987869          0.06410442\nbanana       6.504080 -0.03987869          0.03252630\nbat          6.524272 -0.03987869          0.06333446\nbeaver       6.544039 -0.03987869          0.08449759\nbee          6.514806 -0.03987869          0.05554921\n\n\nNote that these are now much closer to the fixed effects (population-level values). Importantly, see that the slopes that we did not allow to vary are the same across the relevant grouping factor. That’s because we told our model that these effects do not vary between the levels of the relevant grouping factors (e.g., the effect of NativeLanguage does not vary within participant).\n\nVisualisng random effects\nFinally, the point of this blog post: visualising the random effects.\nPrint just the by-subject random effects deviances:\n\ndotplot(ranef(mod_lexdec))$Subject\n\n\n\n\n\n\n\n\nPrint just the by-word random effects:\n\ndotplot(ranef(mod_lexdec))$Word\n\n\n\n\n\n\n\n\nIn these plots, 0 on the x-axis corresponds to the population-level values. So, the (Intercept), 0 corresponds to the intercept value in the model coefficients (6.5408759):\n\nfixef(mod_lexdec)\n\n        (Intercept)           Frequency NativeLanguageOther \n         6.54087589         -0.03987869          0.07859106"
  },
  {
    "objectID": "posts/dotplot/index.html#caterpillar-plot-with-ggplot2",
    "href": "posts/dotplot/index.html#caterpillar-plot-with-ggplot2",
    "title": "Visualising random effects",
    "section": "Caterpillar plot with ggplot2",
    "text": "Caterpillar plot with ggplot2\nWe could also produce these plots by hand. For this we’ll use the broom.mixed package.\n\npacman::p_load(\n  ggplot2,\n  broom.mixed\n)\n\n\nfig_res_dev &lt;-\n  # produce tidy table of random effects\n  broom.mixed::tidy(mod_lexdec, effects = \"ran_vals\", conf.int = TRUE) |&gt; \n  # by-Subject only\n  filter(group == \"Subject\") |&gt; \n  # begin plotting\n  ggplot() +\n  aes(x = estimate, y = reorder(level, estimate))  +\n  labs(title = \"By-participant intercept deviance (log RTs)\",\n       y = \"Participant ID\",\n       x = \"Deviance (log ms)\") +\n  geom_vline(xintercept = 0, colour = \"red\", linetype = \"dashed\") +\n  geom_point(colour = \"blue\") +\n  geom_errorbar(\n    aes(xmin = conf.low,\n        xmax = conf.high)\n  ) +\n  facet_grid(~term) +\n  theme_bw()\n\n\nfig_res_dev\n\n\n\n\n\n\n\nFigure 1: Caterpillar plot of by-participant varying intercepts and slopes produced with broom.mixed::tidy() and ggplot2"
  },
  {
    "objectID": "posts/papapja7/index.html",
    "href": "posts/papapja7/index.html",
    "title": "APA 7 in papaja",
    "section": "",
    "text": "This is a short blogpost documenting the steps I took to get APA 7 running in papaja for Rmarkdown. I followed more detailed instructions in the blogpost How to set up {papaja} to work with the APA 7 format, and also took a look at an open issue on the papaja GitHub repo on this topic."
  },
  {
    "objectID": "posts/papapja7/index.html#setting-up-apa-7-in-papaja",
    "href": "posts/papapja7/index.html#setting-up-apa-7-in-papaja",
    "title": "APA 7 in papaja",
    "section": "Setting up APA 7 in papaja",
    "text": "Setting up APA 7 in papaja\npapaja is an R package for preparing APA-styled articles in Rmarkdown. It’s current version (0.1.2) supports APA 6 guidelines, although the newest guidelines are APA 7. Basically, we need to get papaja to use APA 7.\nI achieved this by following these steps:\n\nInstall developer version of papaja:\n\n\nremotes::install_github(\"crsh/papaja@devel\")\n\n\nAdd to the YAML:\n\n\ncsl               : \"`r system.file('rmd', 'apa7.csl', package = 'papaja')`\"\ndocumentclass     : \"apa7\"\n\nAnd this:\n\nheader-includes:\n  - |\n    \\makeatletter\n    \\renewcommand{\\paragraph}{\\@startsection{paragraph}{4}{\\parindent}%\n      {0\\baselineskip \\@plus 0.2ex \\@minus 0.2ex}%\n      {-1em}%\n      {\\normalfont\\normalsize\\bfseries\\typesectitle}}\n    \n    \\renewcommand{\\subparagraph}[1]{\\@startsection{subparagraph}{5}{1em}%\n      {0\\baselineskip \\@plus 0.2ex \\@minus 0.2ex}%\n      {-\\z@\\relax}%\n      {\\normalfont\\normalsize\\bfseries\\itshape\\hspace{\\parindent}{#1}\\textit{\\addperi}}{\\relax}}\n    \\makeatother\n\nThis was sufficient for my set-up. The additional LaTeX packages I needed were then listed directly under (and in line with) \\makatother."
  },
  {
    "objectID": "posts/papapja7/index.html#testing",
    "href": "posts/papapja7/index.html#testing",
    "title": "APA 7 in papaja",
    "section": "Testing",
    "text": "Testing\nTo test if APA 7 vs. 6 was being used, I changed a BibTex entry for a reference to have more than 7 authors (e.g., by repeating all the author names until &gt;7). I then changed csl: \"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/papaja/rmd/apa7.csl\" to csl: \"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/papaja/rmd/apa6.csl\". In the bibliography only 7 authors were listed (following apa 6). I then changed it back to 'apa7.csl, and the rendered bibliography listed all authors (following apa 7). I took this as sufficient evidence that apa 7 was indeed being used. I then changed the BibTex authors back, of course."
  },
  {
    "objectID": "posts/papapja7/index.html#yaml",
    "href": "posts/papapja7/index.html#yaml",
    "title": "APA 7 in papaja",
    "section": "YAML",
    "text": "YAML\nFor reference, the YAML in my final metafile looks something like this:\n\n---\ntitle             : \"Title\"\nauthor: \n  - name          : \"Daniela Palleschi\"\n\n[...]\n\nbibliography      : [\"references.bib\"]\nfloatsintext      : yes\nfigurelist        : yes\nnumbersections    : yes\ntablelist         : yes\nfootnotelist      : no\nlinenumbers       : yes\nmask              : no\ndraft             : no\ncsl               : \"`r system.file('rmd', 'apa7.csl', package = 'papaja')`\"\ndocumentclass     : \"apa7\"\nclassoption       : \"man\"\nkeep_tex: true\noutput: \n  papaja::apa6_pdf:\n    # citation_package: natbib # so that the TEX file output will contain LaTeX formatted citations\n# natbiboptions: round # so that natbib use parantheses for citations and not square brackets\nbiblio-style: apalike\nauthornote: |\n  \\addORCIDlink{Daniela M. Palleschi}{0000-0002-7633-2736}\nheader-includes:\n- |\n    \\makeatletter\n    \\renewcommand{\\paragraph}{\\@startsection{paragraph}{4}{\\parindent}%\n      {0\\baselineskip \\@plus 0.2ex \\@minus 0.2ex}%\n      {-1em}%\n      {\\normalfont\\normalsize\\bfseries\\typesectitle}}\n    \\renewcommand{\\subparagraph}[1]{\\@startsection{subparagraph}{5}{1em}%\n      {0\\baselineskip \\@plus 0.2ex \\@minus 0.2ex}%\n      {-\\z@\\relax}%\n      {\\normalfont\\normalsize\\bfseries\\itshape\\hspace{\\parindent}{#1}\\textit{\\addperi}}{\\relax}}\n    \\makeatother\n    \\usepackage{float}\n    \\usepackage{multirow}\n    \\usepackage{makecell}\n    \\usepackage{gb4e} \\noautomath\n    \\usepackage{dblfloatfix}\n    \\usepackage{hyperref}\n    \\usepackage{booktabs}\n    \\usepackage{array}\n    \\usepackage{caption}\n    \\captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}\n    \\usepackage{multirow,graphicx}\n    \\usepackage{fdsymbol}\n    \\usepackage{setspace}\n    \\AtBeginEnvironment{tabular}{\\singlespacing}\n    \\AtBeginEnvironment{lltable}{\\singlespacing}\n    \\AtBeginEnvironment{tablenotes}{\\doublespacing}\n    \\captionsetup[table]{font={stretch=1.5}}\n    \\captionsetup[figure]{font={stretch=1.5}}\neditor_options: \n  chunk_output_type: console\n---"
  },
  {
    "objectID": "future-posts/git-with-rproj/index.html",
    "href": "future-posts/git-with-rproj/index.html",
    "title": "GitHub with RProject",
    "section": "",
    "text": "My first blog."
  },
  {
    "objectID": "future-posts/purpose/index.html",
    "href": "future-posts/purpose/index.html",
    "title": "Closing 2023",
    "section": "",
    "text": "It feels strange to be starting a blog dedicated to (lingusitic) data analysis, as like most linguists I’m essentially self-taught in R in that the courses that were offered during my studies did not take me quite as far as I needed to go. As I understand it, this is not something unique to linguistics. As noted in Lawlor et al. (2022) (a highly recommended read), Emery et al. (2021) reported that the vast majority of life science educators surveyed rated data skills as highly important for students, and yet only about half of respondents indicated that such skills were likely to be learned in required or even elective courses, with ‘Course in another department’ receiving the highest overall likelihood rating. I believe we are in a similar situation in the language sciences: For students and researchers to be successful and competitive candidates for future positions/study programs, data skills are important, but they’re not so important that we expect our students will be learning them to satisfaction in our departments? The call is coming from inside the house…\nI say all this as a former linguistics student myself. I see myself in my current students and peers. We started studying linguistics because of the humanities aspect of the field: understanding language, dissecting and developing language theory, observing and describing language patterns and differences between speakers and speaker communities. The last thing we were thinking about was math and computer science, but alas, here I find myself starting a blog on just that topic (or at least a sub-topic). And so, here is one of my intentions for 2024: regularly write mini-tutorials in this blog to (a) help students/peers, (b) map out topics for courses, workshops, or single lectures, and (c) remind future-me how I did something. Though reason (a) might seem totally selfless, when you consider my current position includes teaching and consulting on similar topics, this actually will lighten my workload in the long run, as I won’t have to provide consultations on the same problem over and over again! Reasons (b) and (c) are also pretty self-serving in that they’ll simplify my workflow and productivity. So in conclusion, I’m creating this blog for totally selfish reasons, namely to make my own life easier, and if it helps others in the long run that’s just icing on the cake.\nTo selfishly helping others in 2024, and Auld Lang Syne!\n\n\n\n\n\n\nReferences\n\nEmery, N. C., Crispo, E., Supp, S. R., Farrell, K. J., Kerkhoff, A. J., Bledsoe, E. K., O’Donnell, K. L., McCall, A. C., & Aiello-Lammens, M. E. (2021). Data Science in Undergraduate Life Science Education: A Need for Instructor Skills Training. BioScience, 71(12), 1274–1287. https://doi.org/10.1093/biosci/biab107\n\n\nLawlor, J., Banville, F., Forero-Muñoz, N.-R., Hébert, K., Martínez-Lanfranco, J. A., Rogy, P., & MacDonald, A. A. M. (2022). Ten simple rules for teaching yourself R. PLOS Computational Biology, 18(9), e1010372. https://doi.org/10.1371/journal.pcbi.1010372"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Visualising random effects\n\n\n\n\n\n\nlinear mixed models\n\n\ndata visualisation\n\n\n\nCaterpillar plots with lattice::dotplot()\n\n\n\n\n\nOct 14, 2024\n\n\nDaniela Palleschi\n\n\n\n\n\n\n\n\n\n\n\n\nAPA 7 in papaja\n\n\n\n\n\n\nreproducibility\n\n\nwriting\n\n\n\nSteps to update papaja to APA 7\n\n\n\n\n\nFeb 19, 2024\n\n\nDaniela Palleschi\n\n\n\n\n\n\nNo matching items"
  }
]